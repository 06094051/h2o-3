{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"A simple demo of new RNN cell with PTB language model.\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import rnn\n",
    "from bucket_io import BucketSentenceIter, default_build_vocab\n",
    "\n",
    "\n",
    "# data_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), 'data'))\n",
    "data_dir = '/home/ubuntu/h2o-3/bigdata/laptop/ptb'\n",
    "\n",
    "def Perplexity(label, pred):\n",
    "    # TODO(tofix): we make a transpose of label here, because when\n",
    "    # using the RNN cell, we called swap axis to the data.\n",
    "    label = label.T.reshape((-1,))\n",
    "    loss = 0.\n",
    "    for i in range(pred.shape[0]):\n",
    "        loss += -np.log(max(1e-10, pred[i][int(label[i])]))\n",
    "    return np.exp(loss / label.size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of dataset ==================\n",
      "bucket of len  10 : 19479 samples\n",
      "bucket of len  20 : 19336 samples\n",
      "bucket of len  30 : 12208 samples\n",
      "bucket of len  40 : 3962 samples\n",
      "bucket of len  50 : 845 samples\n",
      "bucket of len  60 : 160 samples\n",
      "Summary of dataset ==================\n",
      "bucket of len  10 : 1531 samples\n",
      "bucket of len  20 : 1518 samples\n",
      "bucket of len  30 : 980 samples\n",
      "bucket of len  40 : 322 samples\n",
      "bucket of len  50 : 65 samples\n",
      "bucket of len  60 : 10 samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    batch_size = 128\n",
    "    buckets = [10, 20, 30, 40, 50, 60]\n",
    "    num_hidden = 200\n",
    "    num_embed = 200\n",
    "    num_lstm_layer = 2\n",
    "\n",
    "    num_epoch = 2\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.0\n",
    "\n",
    "    contexts = [mx.context.gpu(i) for i in range(1)]\n",
    "    vocab = default_build_vocab(os.path.join(data_dir, 'ptb.train.txt'))\n",
    "\n",
    "    init_h = [('LSTM_init_h', (batch_size, num_lstm_layer, num_hidden))]\n",
    "    init_c = [('LSTM_init_c', (batch_size, num_lstm_layer, num_hidden))]\n",
    "    init_states = init_c + init_h\n",
    "\n",
    "    data_train = BucketSentenceIter(os.path.join(data_dir, 'ptb.train.txt'),\n",
    "                                    vocab, buckets, batch_size, init_states)\n",
    "    data_val = BucketSentenceIter(os.path.join(data_dir, 'ptb.valid.txt'),\n",
    "                                  vocab, buckets, batch_size, init_states)\n",
    "\n",
    "    def sym_gen(seq_len):\n",
    "        data = mx.symbol.Variable('data')\n",
    "        label = mx.symbol.Variable('softmax_label')\n",
    "        embed = mx.symbol.Embedding(data=data, input_dim=len(vocab),\n",
    "                                 output_dim=num_embed, name='embed')\n",
    "\n",
    "        # TODO(tofix)\n",
    "        # The inputs and labels from IO are all in batch-major.\n",
    "        # We need to transform them into time-major to use RNN cells.\n",
    "        embed_tm = mx.symbol.SwapAxis(embed, dim1=0, dim2=1)\n",
    "        label_tm = mx.symbol.SwapAxis(label, dim1=0, dim2=1)\n",
    "\n",
    "        # TODO(tofix)\n",
    "        # Create transformed RNN initial states. Normally we do\n",
    "        # no need to do this. But the RNN symbol expects the state\n",
    "        # to be time-major shape layout, while the current mxnet\n",
    "        # IO and high-level training logic assume everything from\n",
    "        # the data iter have batch_size as the first dimension.\n",
    "        # So until we have extended our IO and training logic to\n",
    "        # support this more general case, this dummy axis swap is\n",
    "        # needed.\n",
    "        rnn_h_init = mx.symbol.SwapAxis(mx.symbol.Variable('LSTM_init_h'),\n",
    "                                     dim1=0, dim2=1)\n",
    "        rnn_c_init = mx.symbol.SwapAxis(mx.symbol.Variable('LSTM_init_c'),\n",
    "                                     dim1=0, dim2=1)\n",
    "\n",
    "        # TODO(tofix)\n",
    "        # currently all the LSTM parameters are concatenated as\n",
    "        # a huge vector, and named '<name>_parameters'. By default\n",
    "        # mxnet initializer does not know how to initilize this\n",
    "        # guy because its name does not ends with _weight or _bias\n",
    "        # or anything familiar. Here we just use a temp workaround\n",
    "        # to create a variable and name it as LSTM_bias to get\n",
    "        # this demo running. Note by default bias is initialized\n",
    "        # as zeros, so this is not a good scheme. But calling it\n",
    "        # LSTM_weight is not good, as this is 1D vector, while\n",
    "        # the initialization scheme of a weight parameter needs\n",
    "        # at least two dimensions.\n",
    "        rnn_params = mx.symbol.Variable('LSTM_bias')\n",
    "\n",
    "        # RNN cell takes input of shape (time, batch, feature)\n",
    "        rnn = mx.symbol.RNN(data=embed_tm, state_size=num_hidden,\n",
    "                         num_layers=num_lstm_layer, mode='lstm',\n",
    "                         name='LSTM', \n",
    "                         # The following params can be omitted\n",
    "                         # provided we do not need to apply the\n",
    "                         # workarounds mentioned above\n",
    "                         state=rnn_h_init,\n",
    "                         state_cell=rnn_c_init, \n",
    "                         parameters=rnn_params)\n",
    "\n",
    "        # the RNN cell output is of shape (time, batch, dim)\n",
    "        # if we need the states and cell states in the last time\n",
    "        # step (e.g. when building encoder-decoder models), we\n",
    "        # can set state_outputs=True, and the RNN cell will have\n",
    "        # extra outputs: rnn['LSTM_output'], rnn['LSTM_state']\n",
    "        # and for LSTM, also rnn['LSTM_state_cell']\n",
    "\n",
    "        # now we collapse the time and batch dimension to do the\n",
    "        # final linear logistic regression prediction\n",
    "        hidden = mx.symbol.Reshape(data=rnn, shape=(-1, num_hidden))\n",
    "        label_cl = mx.symbol.Reshape(data=label_tm, shape=(-1,))\n",
    "\n",
    "        pred = mx.symbol.FullyConnected(data=hidden, num_hidden=len(vocab),\n",
    "                                     name='pred')\n",
    "        sm = mx.symbol.SoftmaxOutput(data=pred, label=label_cl, name='softmax')\n",
    "\n",
    "        data_names = ['data', 'LSTM_init_h', 'LSTM_init_c']\n",
    "        label_names = ['softmax_label']\n",
    "\n",
    "        return (sm, data_names, label_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-04 19:46:50,721 Epoch[0] Batch [50]\tSpeed: 1048.01 samples/sec\tTrain-Perplexity=4260.871099\n",
      "2017-01-04 19:46:56,500 Epoch[0] Batch [100]\tSpeed: 1107.60 samples/sec\tTrain-Perplexity=744.343631\n",
      "2017-01-04 19:47:02,942 Epoch[0] Batch [150]\tSpeed: 993.64 samples/sec\tTrain-Perplexity=563.627497\n",
      "2017-01-04 19:47:09,086 Epoch[0] Batch [200]\tSpeed: 1041.86 samples/sec\tTrain-Perplexity=474.455599\n",
      "2017-01-04 19:47:14,218 Epoch[0] Batch [250]\tSpeed: 1247.18 samples/sec\tTrain-Perplexity=307.823897\n",
      "2017-01-04 19:47:19,753 Epoch[0] Batch [300]\tSpeed: 1156.57 samples/sec\tTrain-Perplexity=330.747983\n",
      "2017-01-04 19:47:25,525 Epoch[0] Batch [350]\tSpeed: 1108.93 samples/sec\tTrain-Perplexity=338.972245\n",
      "2017-01-04 19:47:31,344 Epoch[0] Batch [400]\tSpeed: 1099.98 samples/sec\tTrain-Perplexity=337.709651\n",
      "2017-01-04 19:47:34,969 Epoch[0] Train-Perplexity=280.663323\n",
      "2017-01-04 19:47:34,970 Epoch[0] Time cost=50.987\n",
      "2017-01-04 19:47:37,576 Epoch[0] Validation-Perplexity=287.798634\n",
      "2017-01-04 19:47:43,705 Epoch[1] Batch [50]\tSpeed: 1053.36 samples/sec\tTrain-Perplexity=341.520990\n",
      "2017-01-04 19:47:49,443 Epoch[1] Batch [100]\tSpeed: 1115.59 samples/sec\tTrain-Perplexity=292.795156\n",
      "2017-01-04 19:47:55,864 Epoch[1] Batch [150]\tSpeed: 996.73 samples/sec\tTrain-Perplexity=325.625157\n",
      "2017-01-04 19:48:02,001 Epoch[1] Batch [200]\tSpeed: 1043.09 samples/sec\tTrain-Perplexity=311.844707\n",
      "2017-01-04 19:48:07,205 Epoch[1] Batch [250]\tSpeed: 1229.97 samples/sec\tTrain-Perplexity=232.217817\n",
      "2017-01-04 19:48:12,853 Epoch[1] Batch [300]\tSpeed: 1133.27 samples/sec\tTrain-Perplexity=258.266448\n",
      "2017-01-04 19:48:18,640 Epoch[1] Batch [350]\tSpeed: 1106.14 samples/sec\tTrain-Perplexity=273.021966\n",
      "2017-01-04 19:48:24,507 Epoch[1] Batch [400]\tSpeed: 1091.01 samples/sec\tTrain-Perplexity=279.475742\n",
      "2017-01-04 19:48:28,160 Epoch[1] Train-Perplexity=239.810865\n",
      "2017-01-04 19:48:28,161 Epoch[1] Time cost=50.584\n",
      "2017-01-04 19:48:30,789 Epoch[1] Validation-Perplexity=245.307591\n"
     ]
    }
   ],
   "source": [
    "if len(buckets) == 1:\n",
    "    mod = mx.mod.Module(*sym_gen(buckets[0]), context=contexts)\n",
    "else:\n",
    "    mod = mx.mod.BucketingModule(sym_gen, default_bucket_key=data_train.default_bucket_key,\n",
    "                                 context=contexts)\n",
    "\n",
    "import logging\n",
    "head = '%(asctime)-15s %(message)s'\n",
    "logging.basicConfig(level=logging.DEBUG, format=head)\n",
    "\n",
    "mod.fit(data_train, eval_data=data_val, num_epoch=num_epoch,\n",
    "        eval_metric=mx.metric.np(Perplexity),\n",
    "        batch_end_callback=mx.callback.Speedometer(batch_size, 50),\n",
    "        initializer=mx.init.Xavier(factor_type=\"in\", magnitude=2.34),\n",
    "        optimizer='sgd',\n",
    "        optimizer_params={'learning_rate': learning_rate,\n",
    "                          'momentum': momentum, 'wd': 0.00001})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
