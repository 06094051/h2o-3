{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321. connected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>26 mins 08 secs</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.11.0.99999</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>1 month and 4 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>ubuntu</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>2.924 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>2.7.12 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------\n",
       "H2O cluster uptime:         26 mins 08 secs\n",
       "H2O cluster version:        3.11.0.99999\n",
       "H2O cluster version age:    1 month and 4 days\n",
       "H2O cluster name:           ubuntu\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    2.924 Gb\n",
       "H2O cluster total cores:    8\n",
       "H2O cluster allowed cores:  8\n",
       "H2O cluster status:         locked, healthy\n",
       "H2O connection url:         http://localhost:54321\n",
       "H2O connection proxy:\n",
       "Python version:             2.7.12 final\n",
       "--------------------------  ----------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h2o\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import random\n",
    "import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "PATH = os.path.expanduser(\"~/h2o-3/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from h2o.estimators.deepwater import H2ODeepWaterEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bucket_io import BucketSentenceIter\n",
    "from collections import namedtuple\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_df = h2o.import_file(PATH + \"/bigdata/laptop/char-rnn/obama.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from lstm import lstm_unroll, lstm_inference_symbol\n",
    "#from bucket_io import BucketSentenceIter\n",
    "#from rnn_model import LSTMInferenceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read from doc\n",
    "def read_content(path):\n",
    "    with open(path) as ins:\n",
    "        content = ins.read()\n",
    "        return content\n",
    "\n",
    "# Build a vocabulary of what char we have in the content\n",
    "def build_vocab(path):\n",
    "    content = read_content(path)\n",
    "    content = list(content)\n",
    "    idx = 1 # 0 is left for zero-padding\n",
    "    the_vocab = {}\n",
    "    for word in content:\n",
    "        if len(word) == 0:\n",
    "            continue\n",
    "        if not word in the_vocab:\n",
    "            the_vocab[word] = idx\n",
    "            idx += 1\n",
    "    return the_vocab\n",
    "\n",
    "# We will assign each char with a special numerical id\n",
    "def text2id(sentence, the_vocab):\n",
    "    words = list(sentence)\n",
    "    words = [the_vocab[w] for w in words if len(w) > 0]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluation/ Loss Function \n",
    "def Perplexity(label, pred):\n",
    "    label = label.T.reshape((-1,))\n",
    "    loss = 0.\n",
    "    for i in range(pred.shape[0]):\n",
    "        loss += -np.log(max(1e-10, pred[i][int(label[i])]))\n",
    "    return np.exp(loss / label.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build char vocabluary from input\n",
    "vocab = build_vocab(PATH + \"bigdata/laptop/char-rnn/obama.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#data_url = \"http://data.mxnet.io/mxnet/data/char_lstm.zip\"\n",
    "#os.system(\"wget %s\" % data_url)\n",
    "#os.system(\"unzip -o char_lstm.zip\")    \n",
    "#with open('obama.txt', 'r') as f:\n",
    "#    print f.read()[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The batch size for training\n",
    "batch_size = 32\n",
    "# We can support various length input\n",
    "# For this problem, we cut each input sentence to length of 129\n",
    "# So we only need fix length bucket\n",
    "buckets = [129]\n",
    "# hidden unit in LSTM cell\n",
    "num_hidden = 512\n",
    "# embedding dimension, which is, map a char to a 256 dim vector\n",
    "num_embed = 256\n",
    "# number of lstm layer\n",
    "num_lstm_layer = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we will show a quick demo in 2 epoch\n",
    "# and we will see result by training 75 epoch\n",
    "num_epoch = 2\n",
    "# learning rate \n",
    "learning_rate = 0.01\n",
    "# we will use pure sgd without momentum\n",
    "momentum = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we can select multi-gpu for training\n",
    "# for this demo we only use one\n",
    "devs = [mx.context.gpu(i) for i in range(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LSTMState = namedtuple(\"LSTMState\", [\"c\", \"h\"])\n",
    "LSTMParam = namedtuple(\"LSTMParam\", [\"i2h_weight\", \"i2h_bias\",\n",
    "                                     \"h2h_weight\", \"h2h_bias\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm(num_hidden, indata, prev_state, param, seqidx, layeridx, dropout=0.):\n",
    "    \"\"\"LSTM Cell symbol\"\"\"\n",
    "    if dropout > 0.:\n",
    "        indata = mx.sym.Dropout(data=indata, p=dropout)\n",
    "    i2h = mx.sym.FullyConnected(data=indata,\n",
    "                                weight=param.i2h_weight,\n",
    "                                bias=param.i2h_bias,\n",
    "                                num_hidden=num_hidden * 4,\n",
    "                                name=\"t%d_l%d_i2h\" % (seqidx, layeridx))\n",
    "    h2h = mx.sym.FullyConnected(data=prev_state.h,\n",
    "                                weight=param.h2h_weight,\n",
    "                                bias=param.h2h_bias,\n",
    "                                num_hidden=num_hidden * 4,\n",
    "                                name=\"t%d_l%d_h2h\" % (seqidx, layeridx))\n",
    "    gates = i2h + h2h\n",
    "    slice_gates = mx.sym.SliceChannel(gates, num_outputs=4,\n",
    "                                      name=\"t%d_l%d_slice\" % (seqidx, layeridx))\n",
    "    in_gate = mx.sym.Activation(slice_gates[0], act_type=\"sigmoid\")\n",
    "    in_transform = mx.sym.Activation(slice_gates[1], act_type=\"tanh\")\n",
    "    forget_gate = mx.sym.Activation(slice_gates[2], act_type=\"sigmoid\")\n",
    "    out_gate = mx.sym.Activation(slice_gates[3], act_type=\"sigmoid\")\n",
    "    next_c = (forget_gate * prev_state.c) + (in_gate * in_transform)\n",
    "    next_h = out_gate * mx.sym.Activation(next_c, act_type=\"tanh\")\n",
    "    return LSTMState(c=next_c, h=next_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_unroll(num_lstm_layer, seq_len, input_size,\n",
    "                num_hidden, num_embed, num_label, dropout=0.):\n",
    "\n",
    "    embed_weight = mx.sym.Variable(\"embed_weight\")\n",
    "    cls_weight = mx.sym.Variable(\"cls_weight\")\n",
    "    cls_bias = mx.sym.Variable(\"cls_bias\")\n",
    "    param_cells = []\n",
    "    last_states = []\n",
    "    for i in range(num_lstm_layer):\n",
    "        param_cells.append(LSTMParam(i2h_weight=mx.sym.Variable(\"l%d_i2h_weight\" % i),\n",
    "                                     i2h_bias=mx.sym.Variable(\"l%d_i2h_bias\" % i),\n",
    "                                     h2h_weight=mx.sym.Variable(\"l%d_h2h_weight\" % i),\n",
    "                                     h2h_bias=mx.sym.Variable(\"l%d_h2h_bias\" % i)))\n",
    "        state = LSTMState(c=mx.sym.Variable(\"l%d_init_c\" % i),\n",
    "                          h=mx.sym.Variable(\"l%d_init_h\" % i))\n",
    "        last_states.append(state)\n",
    "    assert(len(last_states) == num_lstm_layer)\n",
    "\n",
    "    # embeding layer\n",
    "    data = mx.sym.Variable('data')\n",
    "    label = mx.sym.Variable('softmax_label')\n",
    "    embed = mx.sym.Embedding(data=data, input_dim=input_size,\n",
    "                             weight=embed_weight, output_dim=num_embed, name='embed')\n",
    "    wordvec = mx.sym.SliceChannel(data=embed, num_outputs=seq_len, squeeze_axis=1)\n",
    "\n",
    "    hidden_all = []\n",
    "    for seqidx in range(seq_len):\n",
    "        hidden = wordvec[seqidx]\n",
    "\n",
    "        # stack LSTM\n",
    "        for i in range(num_lstm_layer):\n",
    "            if i == 0:\n",
    "                dp_ratio = 0.\n",
    "            else:\n",
    "                dp_ratio = dropout\n",
    "            next_state = lstm(num_hidden, indata=hidden,\n",
    "                              prev_state=last_states[i],\n",
    "                              param=param_cells[i],\n",
    "                              seqidx=seqidx, layeridx=i, dropout=dp_ratio)\n",
    "            hidden = next_state.h\n",
    "            last_states[i] = next_state\n",
    "        # decoder\n",
    "        if dropout > 0.:\n",
    "            hidden = mx.sym.Dropout(data=hidden, p=dropout)\n",
    "        hidden_all.append(hidden)\n",
    "\n",
    "    hidden_concat = mx.sym.Concat(*hidden_all, dim=0)\n",
    "    pred = mx.sym.FullyConnected(data=hidden_concat, num_hidden=num_label,\n",
    "                                 weight=cls_weight, bias=cls_bias, name='pred')\n",
    "\n",
    "    ################################################################################\n",
    "    # Make label the same shape as our produced data path\n",
    "    # I did not observe big speed difference between the following two ways\n",
    "\n",
    "    label = mx.sym.transpose(data=label)\n",
    "    label = mx.sym.Reshape(data=label, target_shape=(0,))\n",
    "\n",
    "    #label_slice = mx.sym.SliceChannel(data=label, num_outputs=seq_len)\n",
    "    #label = [label_slice[t] for t in range(seq_len)]\n",
    "    #label = mx.sym.Concat(*label, dim=0)\n",
    "    #label = mx.sym.Reshape(data=label, target_shape=(0,))\n",
    "    ################################################################################\n",
    "\n",
    "    sm = mx.sym.SoftmaxOutput(data=pred, label=label, name='softmax')\n",
    "\n",
    "    return sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mxnet_lstm = lstm_unroll(num_lstm_layer, buckets[0], len(vocab) + 1,\n",
    "                       num_hidden=num_hidden, num_embed=num_embed,\n",
    "                       num_label=len(vocab) + 1, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buckets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mxnet_lstm.save(\"/tmp/symbol_lstm-py.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate symbol for a length\n",
    "#def sym_gen(seq_len):\n",
    "#    return lstm_unroll(num_lstm_layer, seq_len, len(vocab) + 1,\n",
    "#                       num_hidden=num_hidden, num_embed=num_embed,\n",
    "#                       num_label=len(vocab) + 1, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initalize states for LSTM\n",
    "init_c = [('l%d_init_c'%l, (batch_size, num_hidden)) for l in range(num_lstm_layer)]\n",
    "init_h = [('l%d_init_h'%l, (batch_size, num_hidden)) for l in range(num_lstm_layer)]\n",
    "init_states = init_c + init_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of dataset ==================\n",
      "bucket of len 129 : 8290 samples\n"
     ]
    }
   ],
   "source": [
    "# we can build an iterator for text\n",
    "data_train = BucketSentenceIter(\"./obama.txt\", vocab, buckets, batch_size,\n",
    "                                init_states, seperate_char='\\n',\n",
    "                                text2id=text2id, read_content=read_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the network symbol\n",
    "#symbol = sym_gen(buckets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train a LSTM network as simple as feedforward network\n",
    "model = mx.model.FeedForward(ctx=devs,\n",
    "                             symbol=symbol,\n",
    "                             num_epoch=num_epoch,\n",
    "                             learning_rate=learning_rate,\n",
    "                             momentum=momentum,\n",
    "                             wd=0.0001,\n",
    "                             initializer=mx.init.Xavier(factor_type=\"in\", magnitude=2.34))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit it\n",
    "model.fit(X=data_train,\n",
    "          eval_metric = mx.metric.np(Perplexity),\n",
    "          batch_end_callback=mx.callback.Speedometer(batch_size, 50),\n",
    "          epoch_end_callback=mx.callback.do_checkpoint(\"obama\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper strcuture for prediction\n",
    "def MakeRevertVocab(vocab):\n",
    "    dic = {}\n",
    "    for k, v in vocab.items():\n",
    "        dic[v] = k\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make input from char\n",
    "def MakeInput(char, vocab, arr):\n",
    "    idx = vocab[char]\n",
    "    tmp = np.zeros((1,))\n",
    "    tmp[0] = idx\n",
    "    arr[:] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function for random sample \n",
    "def _cdf(weights):\n",
    "    total = sum(weights)\n",
    "    result = []\n",
    "    cumsum = 0\n",
    "    for w in weights:\n",
    "        cumsum += w\n",
    "        result.append(cumsum / total)\n",
    "    return result\n",
    "\n",
    "def _choice(population, weights):\n",
    "    assert len(population) == len(weights)\n",
    "    cdf_vals = _cdf(weights)\n",
    "    x = random.random()\n",
    "    idx = bisect.bisect(cdf_vals, x)\n",
    "    return population[idx]\n",
    "\n",
    "# we can use random output or fixed output by choosing largest probability\n",
    "def MakeOutput(prob, vocab, sample=False, temperature=1.):\n",
    "    if sample == False:\n",
    "        idx = np.argmax(prob, axis=1)[0]\n",
    "    else:\n",
    "        fix_dict = [\"\"] + [vocab[i] for i in range(1, len(vocab) + 1)]\n",
    "        scale_prob = np.clip(prob, 1e-6, 1 - 1e-6)\n",
    "        rescale = np.exp(np.log(scale_prob) / temperature)\n",
    "        rescale[:] /= rescale.sum()\n",
    "        return _choice(fix_dict, rescale[0, :])\n",
    "    try:\n",
    "        char = vocab[idx]\n",
    "    except:\n",
    "        char = ''\n",
    "    return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load from check-point\n",
    "_, arg_params, __ = mx.model.load_checkpoint(\"obama\", 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build an inference model\n",
    "model = LSTMInferenceModel(num_lstm_layer, len(vocab) + 1,\n",
    "                           num_hidden=num_hidden, num_embed=num_embed,\n",
    "                           num_label=len(vocab) + 1, arg_params=arg_params, ctx=mx.gpu(), dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate a sequence of 1200 chars\n",
    "\n",
    "seq_length = 1200\n",
    "input_ndarray = mx.nd.zeros((1,))\n",
    "revert_vocab = MakeRevertVocab(vocab)\n",
    "# Feel free to change the starter sentence\n",
    "output ='The joke'\n",
    "random_sample = True\n",
    "new_sentence = True\n",
    "\n",
    "ignore_length = len(output)\n",
    "\n",
    "for i in range(seq_length):\n",
    "    if i <= ignore_length - 1:\n",
    "        MakeInput(output[i], vocab, input_ndarray)\n",
    "    else:\n",
    "        MakeInput(output[-1], vocab, input_ndarray)\n",
    "    prob = model.forward(input_ndarray, new_sentence)\n",
    "    new_sentence = False\n",
    "    next_char = MakeOutput(prob, revert_vocab, random_sample)\n",
    "    if next_char == '':\n",
    "        new_sentence = True\n",
    "    if i >= ignore_length - 1:\n",
    "        output += next_char\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's see what we can learned from char in Obama's speech.\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
