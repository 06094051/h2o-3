{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import random\n",
    "import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lstm import lstm_unroll, lstm_inference_symbol\n",
    "from bucket_io import BucketSentenceIter\n",
    "from rnn_model import LSTMInferenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read from doc\n",
    "def read_content(path):\n",
    "    with open(path) as ins:\n",
    "        content = ins.read()\n",
    "        return content\n",
    "\n",
    "# Build a vocabulary of what char we have in the content\n",
    "def build_vocab(path):\n",
    "    content = read_content(path)\n",
    "    content = list(content)\n",
    "    idx = 1 # 0 is left for zero-padding\n",
    "    the_vocab = {}\n",
    "    for word in content:\n",
    "        if len(word) == 0:\n",
    "            continue\n",
    "        if not word in the_vocab:\n",
    "            the_vocab[word] = idx\n",
    "            idx += 1\n",
    "    return the_vocab\n",
    "\n",
    "# We will assign each char with a special numerical id\n",
    "def text2id(sentence, the_vocab):\n",
    "    words = list(sentence)\n",
    "    words = [the_vocab[w] for w in words if len(w) > 0]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluation \n",
    "def Perplexity(label, pred):\n",
    "    label = label.T.reshape((-1,))\n",
    "    loss = 0.\n",
    "    for i in range(pred.shape[0]):\n",
    "        loss += -np.log(max(1e-10, pred[i][int(label[i])]))\n",
    "    return np.exp(loss / label.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call to Renewal Keynote Address Call to Renewal Pt 1Call to Renewal Part 2 TOPIC: Our Past, Our Future & Vision for America June \n",
      "28, 2006 Call to Renewal' Keynote Address Complete Text Good morning. I appreciate the opportunity to speak here at the Call to R\n",
      "enewal's Building a Covenant for a New America conference. I've had the opportunity to take a look at your Covenant for a New Ame\n",
      "rica. It is filled with outstanding policies and prescriptions for much of what ails this country. So I'd like to congratulate yo\n",
      "u all on the thoughtful presentations you've given so far about poverty and justice in America, and for putting fire under the fe\n",
      "et of the political leadership here in Washington.But today I'd like to talk about the connection between religion and politics a\n",
      "nd perhaps offer some thoughts about how we can sort through some of the often bitter arguments that we've been seeing over the l\n",
      "ast several years.I do so because, as you all know, we can affirm the importance of povert\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data_url = \"http://data.mxnet.io/mxnet/data/char_lstm.zip\"\n",
    "os.system(\"wget %s\" % data_url)\n",
    "os.system(\"unzip -o char_lstm.zip\")\n",
    "with open('obama.txt', 'r') as f:\n",
    "    print f.read()[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The batch size for training\n",
    "batch_size = 32\n",
    "# We can support various length input\n",
    "# For this problem, we cut each input sentence to length of 129\n",
    "# So we only need fix length bucket\n",
    "buckets = [129]\n",
    "# hidden unit in LSTM cell\n",
    "num_hidden = 512\n",
    "# embedding dimension, which is, map a char to a 256 dim vector\n",
    "num_embed = 256\n",
    "# number of lstm layer\n",
    "num_lstm_layer = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we will show a quick demo in 2 epoch\n",
    "# and we will see result by training 75 epoch\n",
    "num_epoch = 2\n",
    "# learning rate \n",
    "learning_rate = 0.01\n",
    "# we will use pure sgd without momentum\n",
    "momentum = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we can select multi-gpu for training\n",
    "# for this demo we only use one\n",
    "devs = [mx.context.gpu(i) for i in range(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build char vocabluary from input\n",
    "vocab = build_vocab(\"./obama.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate symbol for a length\n",
    "def sym_gen(seq_len):\n",
    "    return lstm_unroll(num_lstm_layer, seq_len, len(vocab) + 1,\n",
    "                       num_hidden=num_hidden, num_embed=num_embed,\n",
    "                       num_label=len(vocab) + 1, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initalize states for LSTM\n",
    "init_c = [('l%d_init_c'%l, (batch_size, num_hidden)) for l in range(num_lstm_layer)]\n",
    "init_h = [('l%d_init_h'%l, (batch_size, num_hidden)) for l in range(num_lstm_layer)]\n",
    "init_states = init_c + init_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of dataset ==================\n",
      "bucket of len 129 : 8290 samples\n"
     ]
    }
   ],
   "source": [
    "# we can build an iterator for text\n",
    "data_train = BucketSentenceIter(\"./obama.txt\", vocab, buckets, batch_size,\n",
    "                                init_states, seperate_char='\\n',\n",
    "                                text2id=text2id, read_content=read_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the network symbol\n",
    "symbol = sym_gen(buckets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train a LSTM network as simple as feedforward network\n",
    "model = mx.model.FeedForward(ctx=devs,\n",
    "                             symbol=symbol,\n",
    "                             num_epoch=num_epoch,\n",
    "                             learning_rate=learning_rate,\n",
    "                             momentum=momentum,\n",
    "                             wd=0.0001,\n",
    "                             initializer=mx.init.Xavier(factor_type=\"in\", magnitude=2.34))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:29:56 INFO:Start training with [gpu(0)]\n",
      "07:30:44 INFO:Epoch[0] Batch [50]\tSpeed: 33.92 samples/sec\tTrain-Perplexity=28.613187\n",
      "07:31:32 INFO:Epoch[0] Batch [100]\tSpeed: 33.07 samples/sec\tTrain-Perplexity=22.136915\n",
      "07:32:21 INFO:Epoch[0] Batch [150]\tSpeed: 33.19 samples/sec\tTrain-Perplexity=21.647440\n",
      "07:33:09 INFO:Epoch[0] Batch [200]\tSpeed: 33.18 samples/sec\tTrain-Perplexity=21.489897\n",
      "07:33:57 INFO:Epoch[0] Batch [250]\tSpeed: 33.20 samples/sec\tTrain-Perplexity=21.555268\n",
      "07:34:06 INFO:Epoch[0] Resetting Data Iterator\n",
      "07:34:06 INFO:Epoch[0] Time cost=249.182\n",
      "07:34:06 INFO:Saved checkpoint to \"obama-0001.params\"\n",
      "07:34:54 INFO:Epoch[1] Batch [50]\tSpeed: 33.89 samples/sec\tTrain-Perplexity=21.513019\n",
      "07:35:42 INFO:Epoch[1] Batch [100]\tSpeed: 33.21 samples/sec\tTrain-Perplexity=21.366941\n",
      "07:36:30 INFO:Epoch[1] Batch [150]\tSpeed: 33.17 samples/sec\tTrain-Perplexity=21.313107\n",
      "07:37:19 INFO:Epoch[1] Batch [200]\tSpeed: 33.11 samples/sec\tTrain-Perplexity=21.250228\n",
      "07:38:07 INFO:Epoch[1] Batch [250]\tSpeed: 33.15 samples/sec\tTrain-Perplexity=21.085335\n",
      "07:38:16 INFO:Epoch[1] Resetting Data Iterator\n",
      "07:38:16 INFO:Epoch[1] Time cost=249.238\n",
      "07:38:16 INFO:Saved checkpoint to \"obama-0002.params\"\n"
     ]
    }
   ],
   "source": [
    "# Fit it\n",
    "model.fit(X=data_train,\n",
    "          eval_metric = mx.metric.np(Perplexity),\n",
    "          batch_end_callback=mx.callback.Speedometer(batch_size, 50),\n",
    "          epoch_end_callback=mx.callback.do_checkpoint(\"obama\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper strcuture for prediction\n",
    "def MakeRevertVocab(vocab):\n",
    "    dic = {}\n",
    "    for k, v in vocab.items():\n",
    "        dic[v] = k\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make input from char\n",
    "def MakeInput(char, vocab, arr):\n",
    "    idx = vocab[char]\n",
    "    tmp = np.zeros((1,))\n",
    "    tmp[0] = idx\n",
    "    arr[:] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function for random sample \n",
    "def _cdf(weights):\n",
    "    total = sum(weights)\n",
    "    result = []\n",
    "    cumsum = 0\n",
    "    for w in weights:\n",
    "        cumsum += w\n",
    "        result.append(cumsum / total)\n",
    "    return result\n",
    "\n",
    "def _choice(population, weights):\n",
    "    assert len(population) == len(weights)\n",
    "    cdf_vals = _cdf(weights)\n",
    "    x = random.random()\n",
    "    idx = bisect.bisect(cdf_vals, x)\n",
    "    return population[idx]\n",
    "\n",
    "# we can use random output or fixed output by choosing largest probability\n",
    "def MakeOutput(prob, vocab, sample=False, temperature=1.):\n",
    "    if sample == False:\n",
    "        idx = np.argmax(prob, axis=1)[0]\n",
    "    else:\n",
    "        fix_dict = [\"\"] + [vocab[i] for i in range(1, len(vocab) + 1)]\n",
    "        scale_prob = np.clip(prob, 1e-6, 1 - 1e-6)\n",
    "        rescale = np.exp(np.log(scale_prob) / temperature)\n",
    "        rescale[:] /= rescale.sum()\n",
    "        return _choice(fix_dict, rescale[0, :])\n",
    "    try:\n",
    "        char = vocab[idx]\n",
    "    except:\n",
    "        char = ''\n",
    "    return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load from check-point\n",
    "_, arg_params, __ = mx.model.load_checkpoint(\"obama\", 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build an inference model\n",
    "model = LSTMInferenceModel(num_lstm_layer, len(vocab) + 1,\n",
    "                           num_hidden=num_hidden, num_embed=num_embed,\n",
    "                           num_label=len(vocab) + 1, arg_params=arg_params, ctx=mx.gpu(), dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate a sequence of 1200 chars\n",
    "\n",
    "seq_length = 1200\n",
    "input_ndarray = mx.nd.zeros((1,))\n",
    "revert_vocab = MakeRevertVocab(vocab)\n",
    "# Feel free to change the starter sentence\n",
    "output ='The joke'\n",
    "random_sample = True\n",
    "new_sentence = True\n",
    "\n",
    "ignore_length = len(output)\n",
    "\n",
    "for i in range(seq_length):\n",
    "    if i <= ignore_length - 1:\n",
    "        MakeInput(output[i], vocab, input_ndarray)\n",
    "    else:\n",
    "        MakeInput(output[-1], vocab, input_ndarray)\n",
    "    prob = model.forward(input_ndarray, new_sentence)\n",
    "    new_sentence = False\n",
    "    next_char = MakeOutput(prob, revert_vocab, random_sample)\n",
    "    if next_char == '':\n",
    "        new_sentence = True\n",
    "    if i >= ignore_length - 1:\n",
    "        output += next_char\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jokes of all years out and may help based on a celebration of my father.She knows that the bottom line is that this isn't just line whatever we can take on their background.\"And so what that's the absule of history. Dishredic automobile that you can realize that in any room left comes harm and bargained about who you believe - now that we have health care, participals. And that's not the change we are being half to say what we have with more perfect, keepating oit.And we have seen its respect. And so I propublic many areas, all her life, or that March made us to reduce our infension, they raise against 9 College or Leagon commission about what you go away.We believe government can change a firespon out of the pols looking to America's individual rights and bringing this debate.I know that the men and women who also moved the great E85 for every waite death, major challenges positionstanced over the last few years, she should race here directly with \"today, a hunger that says \"read to is your true story. And she saw house and convention describes organizers are giving their money with the nold safety that one of the cost of challenging community shared hard strigght befor\n"
     ]
    }
   ],
   "source": [
    "# Let's see what we can learned from char in Obama's speech.\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
