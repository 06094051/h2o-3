{
  "version": "1.0.0",
  "cells": [
    {
      "type": "md",
      "input": "### Arno Candel, PhD, Chief Architect, H2O.ai\n\nIn this tutorial, we show how to build a well-tuned H2O GBM model for a supervised classification task. We specifically don't focus on feature engineering and use a small dataset to allow you to reproduce these results in a few minutes on a laptop. This script can be directly transferred to datasets that are hundreds of GBs large and H2O clusters with dozens of compute nodes.\n\nThis tutorial is also available in R and Python:\n* R users can download a [R Markdown](http://rmarkdown.rstudio.com) [from H2O's github repository](https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/tutorials/gbm/gbmTuning.Rmd).\n* Python users can download a Jupyter Notebook from [Add Link]\n\n## Installation & Launch of H2O for Flow\nEither download H2O from [H2O.ai's website](http://h2o.ai/download) or install the latest version of H2O using the following command line code:\n\n1. [Download H2O](http://download.h2o.ai/download/h2o-3.8.2.6?id=2d95c79a-3996-6528-bc67-0a46e6152fba&_ga=1.200954990.1337050782.1458593616). This is a zip file that contains everything you need to get started.\n\n    Or run the following from your command line:\n         curl -o h2o.zip http://download.h2o.ai/versions/h2o-3.8.2.3.zip\n2. From your terminal, run:\n         cd ~/Downloads\n         unzip h2o-3.8.2.6.zip\n         cd h2o-3.8.2.6\n         java -jar h2o.jar\n3. Point your browser to http://localhost:54321\n\n4. The next time you want to launch Flow, change into the directory that contains your H2O package from the command line, and run the JAR file.\n    *(note: if your H2O package is not in the Downloads folder, replace the following path  ~/Downloads/h2o-3.8.2.3 with the correct path to your h2o-3.8.2.3 package)*:\n         cd ~/Downloads/h2o-3.8.2.3 \n         java -jar h2o.jar\n\n## Import the data into H2O \nEverything is scalable and distributed from now on. All processing is done on the fully multi-threaded and distributed H2O Java-based backend and can be scaled to large datasets on large compute clusters.\nHere, we use a small public dataset ([Titanic](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/Titanic.html)), but you can use datasets that are hundreds of GBs large.\n"
    },
    {
      "type": "cs",
      "input": "## 'path' can point to a local file, hdfs, s3, nfs, Hive, directories, etc.\nimportFiles\n# in the `Search` field enter http://s3.amazonaws.com/h2o-public-test-data/smalldata/gbm_test/titanic.csv\n# hit enter to add the file, click on the file to select, and then click on `Import`"
    },
    {
      "type": "cs",
      "input": "importFiles [ \"http://s3.amazonaws.com/h2o-public-test-data/smalldata/gbm_test/titanic.csv\" ]\n# click on `Parse these files...`"
    },
    {
      "type": "cs",
      "input": "setupParse paths: [ \"http://s3.amazonaws.com/h2o-public-test-data/smalldata/gbm_test/titanic.csv\" ]\n# select `enum` from the `survived` response column's dropdown menu  to convert it from an integer to a categorical factor\n# then click `Parse`"
    },
    {
      "type": "cs",
      "input": "parseFiles\n  paths: [\"http://s3.amazonaws.com/h2o-public-test-data/smalldata/gbm_test/titanic.csv\"]\n  destination_frame: \"titanic.hex\"\n  parse_type: \"CSV\"\n  separator: 44\n  number_columns: 14\n  single_quotes: false\n  column_names: [\"pclass\",\"survived\",\"name\",\"sex\",\"age\",\"sibsp\",\"parch\",\"ticket\",\"fare\",\"cabin\",\"embarked\",\"boat\",\"body\",\"home.dest\"]\n  column_types: [\"Numeric\",\"Enum\",\"String\",\"Enum\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Enum\",\"Enum\",\"Numeric\",\"Numeric\",\"Enum\"]\n  delete_on_done: true\n  check_header: 1\n  chunk_size: 4194304"
    },
    {
      "type": "cs",
      "input": "getFrameSummary \"titanic.hex\"\n\n# click on `Split` to split your data into a train, validation, and test set\n# once you have created your split, click on `Build Model...`\n# within the `Build Model...` cell you will be able to select which set or sets of your data you would like to use"
    },
    {
      "type": "md",
      "input": "From now on, everything is generic and directly applies to most datasets. We assume that all feature engineering is done at this stage and focus on model tuning. For multi-class problems, you can use `h2o.logloss()` or `h2o.confusionMatrix()` instead of `h2o.auc()` and for regression problems, you can use `h2o.deviance()` or `h2o.mse()`.\n\n## Split the data for Machine Learning\nWe split the data into three pieces: 60% for training, 20% for validation, 20% for final testing. \nHere, we use random splitting, but this assumes i.i.d. data. If this is not the case (e.g., when events span across multiple rows or data has a time structure), you'll have to sample your data non-randomly."
    },
    {
      "type": "h6",
      "input": "assist splitFrame, \"titanic.hex\""
    },
    {
      "type": "cs",
      "input": "assist splitFrame, \"titanic.hex\"\n# if needed click on `Add a new split` to add a testing set split\n# enter in the ratio 0.60, 0.20. 0.20 for training, validation, and testing sets respectively\n# then click on `Create` to create you splits\n# you can leave the seed as the number provided or change it to 1234 (to make the seed easier to remember if reused)"
    },
    {
      "type": "cs",
      "input": "splitFrame \"titanic.hex\", [0.2,0.2], [\"titanic_validation.hex_0.20\",\"titanic_testing.hex_0.20\",\"titanic_training.hex_0.60\"], 1234"
    },
    {
      "type": "md",
      "input": "## Establish baseline performance\nAs the first step, we'll build some default models to see what accuracy we can expect. Let's use the [AUC metric](http://mlwiki.org/index.php/ROC_Analysis) for this demo, but you can use `h2o.logloss` and `stopping_metric=\"logloss\"` as well. It ranges from 0.5 for random models to 1 for perfect models.\n\n\nThe first model is a default GBM, trained on the 60% training split"
    },
    {
      "type": "cs",
      "input": "assist buildModel, null, training_frame: \"titanic.hex\"\n# select `titanic_training.hex_0.60` from the `training_frame` dropdown menu\n# select `titanic_validation.hex_0.20` from the `validation_frame` dropdown menu\n# select `survived` from the `response_column` dropdown menu\n# click the box next to `name` in the table for `ignored_columns`\n# leave everything else as the default value provided"
    },
    {
      "type": "cs",
      "input": "buildModel 'gbm', {\"model_id\":\"gbm-5eebd260-5a88-4453-bcf4-36d7b72748f0\",\"training_frame\":\"titanic_training.hex_0.60\",\"validation_frame\":\"titanic_validation.hex_0.20\",\"nfolds\":0,\"response_column\":\"survived\",\"ignored_columns\":[\"name\"],\"ignore_const_cols\":true,\"ntrees\":50,\"max_depth\":5,\"min_rows\":10,\"nbins\":20,\"seed\":-1,\"learn_rate\":0.1,\"distribution\":\"AUTO\",\"sample_rate\":1,\"col_sample_rate\":1,\"score_each_iteration\":false,\"score_tree_interval\":0,\"nbins_top_level\":1024,\"nbins_cats\":1024,\"r2_stopping\":0.999999,\"stopping_rounds\":0,\"stopping_metric\":\"AUTO\",\"stopping_tolerance\":0.001,\"max_runtime_secs\":0,\"learn_rate_annealing\":1,\"checkpoint\":\"\",\"col_sample_rate_per_tree\":1,\"min_split_improvement\":0,\"histogram_type\":\"AUTO\",\"build_tree_one_node\":false,\"sample_rate_per_class\":[],\"col_sample_rate_change_per_level\":1,\"max_abs_leafnode_pred\":1.7976931348623157e+308}"
    },
    {
      "type": "cs",
      "input": "getModel \"gbm-01f08729-12bd-41dc-8b13-c823725ecbf5\"\n# Then click on `Predict`"
    },
    {
      "type": "cs",
      "input": "parseFiles\n  paths: [\"http://s3.amazonaws.com/h2o-public-test-data/smalldata/gbm_test/titanic.csv\"]\n  destination_frame: \"titanic.hex\"\n  parse_type: \"CSV\"\n  separator: 44\n  number_columns: 14\n  single_quotes: false\n  column_names: [\"pclass\",\"survived\",\"name\",\"sex\",\"age\",\"sibsp\",\"parch\",\"ticket\",\"fare\",\"cabin\",\"embarked\",\"boat\",\"body\",\"home.dest\"]\n  column_types: [\"Numeric\",\"Enum\",\"String\",\"Enum\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Enum\",\"Enum\",\"Numeric\",\"Numeric\",\"Enum\"]\n  delete_on_done: true\n  check_header: 1\n  chunk_size: 4194304"
    },
    {
      "type": "cs",
      "input": "predict model: \"gbm-01f08729-12bd-41dc-8b13-c823725ecbf5\"\n# select `titanic_validation.hex_0.20` from the `Frame` dropdown menu\n# then click on `Predict`"
    },
    {
      "type": "cs",
      "input": "predict model: \"gbm-01f08729-12bd-41dc-8b13-c823725ecbf5\", frame: \"titanic_validation.hex_0.20\", predictions_frame: \"prediction-fd707925-da01-4c90-994d-ee29f4409527\""
    },
    {
      "type": "cs",
      "input": "getFrameSummary \"prediction-fd707925-da01-4c90-994d-ee29f4409527\"\n# click on `View Data` to see the prediction output"
    },
    {
      "type": "cs",
      "input": "getFrameData \"prediction-fd707925-da01-4c90-994d-ee29f4409527\""
    },
    {
      "type": "md",
      "input": "## Get the AUC on the validation set\nThe AUC is over 94%, so this model is highly predictive!"
    },
    {
      "type": "md",
      "input": "The second model is another default GBM, but trained on 80% of the data (here, we combine the training and validation splits to get more training data), and cross-validated using 4 folds. Note that cross-validation takes longer and is not usually done for really large datasets."
    },
    {
      "type": "cs",
      "input": "splitFrame\n# split your training set into a training and testing set with an 80/20 split\n# use the same seed as your first split 1234\n# click on `Create`"
    },
    {
      "type": "cs",
      "input": "splitFrame \"titanic.hex\", [0.2], [\"titanic.hex_0.20\",\"titanic.hex_0.80\"], 1234"
    },
    {
      "type": "md",
      "input": "## Show a detailed summary of the cross validation metrics\n## This gives you an idea of the variance between the folds\ngbm@model$cross_validation_metrics_summary\n (is the below correct, or is it not the `keep_cross_validation_predictions`)\n\n## How do you do the following:\n## Get the cross-validated AUC by scoring the combined holdout predictions.\n## (Instead of taking the average of the metrics across the folds)\nh2o.auc(h2o.performance(gbm, xval = TRUE))"
    },
    {
      "type": "cs",
      "input": "buildModel\n# selec titanic.hex_80 from the `training_frame` dropdown menu\n# enter 4 for the `nfolds` value\n# select `survived` from the `response_column` dropdown menu\n# click the box next to `name` in the table for `ignored_columns`\n# leave everything else as the default value provided\n# in the `Expert` section check the `keep_cross_validation_predictions` box"
    },
    {
      "type": "cs",
      "input": "buildModel 'gbm', {\"model_id\":\"gbm-310a4aae-9977-445b-8375-02b07c1148a8\",\"training_frame\":\"titanic.hex_0.80\",\"nfolds\":\"4\",\"response_column\":\"survived\",\"ignored_columns\":[\"name\"],\"ignore_const_cols\":true,\"ntrees\":50,\"max_depth\":5,\"min_rows\":10,\"nbins\":20,\"seed\":-1,\"learn_rate\":0.1,\"distribution\":\"AUTO\",\"sample_rate\":1,\"col_sample_rate\":1,\"score_each_iteration\":false,\"score_tree_interval\":0,\"fold_assignment\":\"AUTO\",\"nbins_top_level\":1024,\"nbins_cats\":1024,\"r2_stopping\":0.999999,\"stopping_rounds\":0,\"stopping_metric\":\"AUTO\",\"stopping_tolerance\":0.001,\"max_runtime_secs\":0,\"learn_rate_annealing\":1,\"checkpoint\":\"\",\"col_sample_rate_per_tree\":1,\"min_split_improvement\":0,\"histogram_type\":\"AUTO\",\"keep_cross_validation_predictions\":true,\"keep_cross_validation_fold_assignment\":false,\"build_tree_one_node\":false,\"sample_rate_per_class\":[],\"col_sample_rate_change_per_level\":1,\"max_abs_leafnode_pred\":1.7976931348623157e+308}"
    },
    {
      "type": "cs",
      "input": "getModel \"gbm-310a4aae-9977-445b-8375-02b07c1148a8\""
    },
    {
      "type": "cs",
      "input": "getFrameSummary \"prediction_gbm-310a4aae-9977-445b-8375-02b07c1148a8_cv_1\""
    },
    {
      "type": "md",
      "input": "We see that the cross-validated performance is similar to the validation set performance:\n[1] 0.9403432\n\nNext, we train a GBM with \"I feel lucky\" parameters. We'll use early stopping to automatically tune the number of trees using the validation AUC. We'll use a lower learning rate (lower is always better, just takes more trees to converge). We'll also use stochastic sampling of rows and columns to (hopefully) improve generalization."
    },
    {
      "type": "cs",
      "input": "buildModel\n# select `titanic_training.hex_0.60` from the `training_frame` dropdown menu\n# select `titanic_validation.hex_0.20` from the `validation_frame` dropdown menu\n# select `survived` from the `response_column` dropdown menu\n# click the box next to `name` in the table for `ignored_columns`\n\n## more trees is better if the learning rate is small enough \n## here, use \"more than enough\" trees - we have early stopping\nntrees = 10000      \n\n## fix a random number generator seed for reproducibility\nseed = 1234 \n                                \n## smaller learning rate is better (this is a good value for most datasets, but see below for annealing)\nlearn_rate = 0.01\n\n## sample 80% of rows per tree\nsample_rate = 0.8 \n    \n## sample 80% of columns per split\ncol_sample_rate = 0.8\n\n## early stopping once the validation AUC doesn't improve by at least 0.01% for 5 consecutive scoring events\nstopping_rounds = 5, stopping_tolerance = 1e-4, stopping_metric = \"AUC\"\n\n## score every 10 trees to make early stopping reproducible (it depends on the scoring interval)\n  score_tree_interval = 10                                                 \n)"
    }
  ]
}