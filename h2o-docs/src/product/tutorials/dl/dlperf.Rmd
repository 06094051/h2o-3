## Introduction

This tutorial demonstrates basic performance tuning principles for H2O Deep Learning.

Please consult the [H2O documentation](http://h2o.ai), [H2O World Training](http://learn.h2o.ai) and the [H2O Booklets](https://leanpub.com/u/h2oai) for detailed instructions on H2O and H2O Deep Learning. There are many [YouTube videos](https://www.youtube.com/user/0xdata) and [Slide decks](http://www.slideshare.net/0xdata/presentations?order=popular) on H2O Deep Learning. You can also follow me on [@ArnoCandel](http://twitter.com/ArnoCandel) to stay up-to-date. This tutorial does not attempt to explain the various hyper-parameters of H2O Deep Learning. You can [download and execute this script](https://github.com/h2oai/h2o-dev/blob/master/h2o-docs/src/product/tutorials/dl/dlperf.Rmd), however, which will automatically download H2O, launch a single-node H2O cluster on your laptop or workstation and run the benchmarks. The script is both valid R and markdown code, and should run out of the box for you in RStudio (download, open in RStudio, install `rmarkdown` package, then click on `Chunks` -> `Run All`). It was also used to create this blog via a [simple bash script](https://github.com/h2oai/h2o-dev/blob/master/h2o-docs/src/product/tutorials/dl/go.sh). Please [contact me](arno@h2o.ai) if you have questions or comments.

```{r}
options(repos=structure(c(CRAN='http://cran.us.r-project.org')))
install.packages("rmarkdown")
```

### Distributed Benchmarking - Coming Soon
As many of you might know, distributed benchmarking has its own issues and complexity. I've burned many millions of core hours on the world's fastest supercomputers, so I have a good sense of what it takes to get code to scale. Network latencies, bandwidths and model parameters greatly affect the performance on distributed systems. This benchmark script can seemlessly run on a large multi-node H2O cluster (where some parameters would benefit from some changes), and the scalability of H2O Deep Learning can be fully tuned from close-to-linear speedup with very little communication to totally communication-dominated regimes. The current implemention automatically attemps to balance computation and communication with dynamic auto-tuning during runtime. Due to the complexity involved, we refer to a later blog on distributed benchmarking of H2O Deep Learning, so please stay tuned for that.

## Part 1 - Single-Node Training Speed
For simplicity, we start with some single-node experiments quantifying the raw training speed. Of course, many of the parameters we'll change affect the predictive accuracy of a model, but for this first part, and to get an idea of how H2O Deep Learning works, we only look the speed of training (training samples per time).

### Establish Connection to H2O
For this study, we use the latest version of H2O, [h2o-dev](http://github.com/h2oai/h2o-dev). Its numerical performance is very similar to the performance of [h2o](http://github.com/h2oai/h2o). Instructions for installation and execution in stand-alone mode, R, Python, Hadoop or Spark environments can be found at [h2o.ai/download](http://h2o.ai/download), but you can just follow this script from R, and everything should just work.

We use nightly build 1072 of h2o-dev, but you should be able to use the latest nightly or latest stable version as well.

The following two commands remove any previously installed H2O packages for R.
```{r}
if ("package:h2o" %in% search()) { detach("package:h2o", unload=TRUE) }
if ("h2o" %in% rownames(installed.packages())) { remove.packages("h2o") }
```

Next, we download packages that H2O depends on.
```{r}
if (! ("methods" %in% rownames(installed.packages()))) { install.packages("methods") }
if (! ("statmod" %in% rownames(installed.packages()))) { install.packages("statmod") }
if (! ("stats" %in% rownames(installed.packages()))) { install.packages("stats") }
if (! ("graphics" %in% rownames(installed.packages()))) { install.packages("graphics") }
if (! ("RCurl" %in% rownames(installed.packages()))) { install.packages("RCurl") }
if (! ("rjson" %in% rownames(installed.packages()))) { install.packages("rjson") }
if (! ("tools" %in% rownames(installed.packages()))) { install.packages("tools") }
if (! ("utils" %in% rownames(installed.packages()))) { install.packages("utils") }
```
Now we download, install and initialize the H2O package for R.
```{r}
install.packages("h2o", type="source", repos=(c("http://h2o-release.s3.amazonaws.com/h2o-dev/master/1072/R")))
library(h2o)
library('stringr')
```
Start a single-node instance of H2O using all available processor cores and reserve 8GB of memory (less should work too, but check your logs for `WARN: Pausing to swap to disk; more memory may help` using the Flow GUI at [localhost:54321](http://localhost:54321), `Admin` -> `View Log` -> `SELECT LOG FILE TYPE: warn`).
```{r}
h2oServer <- h2o.init(ip="localhost", port=54321, max_mem_size="8g", nthreads=-1)
#h2oServer <- h2o.init(ip="h2o-cluster", port=54321) # optional: connect to running H2O cluster
```

We will need to set the path to a scratch directory that will contain the downloaded datasets and the CSV files containing the results. Please adjust at your will.
```{r}
workdir="/tmp"
```

###Load Benchmark Dataset into H2O's Memory

Next, we download the well-known [MNIST](http://yann.lecun.com/exdb/mnist/) dataset of hand-written digits, where each row contains the 28^2=784 gray-scale pixel values from 0 to 255 of digitized digits (last column contains labels from 0 to 9). The training and testing datasets are also available at [H2O Github](https://github.com/h2oai/h2o/tree/master/smalldata/mnist).

We are well aware that this is small data for today's standards, but since H2O Deep Learning is a fully distributed streaming algorithm that processes point-by-point, its performance in terms of throughput will remain similar for much larger datasets as each processing core streams through training samples (i.e., image, data point, row) one at a time. The maximum dataset size that can be handled is given by total aggregate amount of memory of all nodes and the compression ratio of the dataset (H2O does lossless compression).
        
We upload the MNIST datasets (train and test sets) into H2O's memory. Keep in mind that R just serves as a remote control for this entire experiment and never does any actual computation. All the heavy lifting is done by the fully parallelized H2O in-memory engine.
```{r}
download.file("https://s3.amazonaws.com/h2o-training/mnist/train.csv.gz", file.path(workdir,"train.csv.gz"), "wget", quiet = T)
download.file("https://s3.amazonaws.com/h2o-training/mnist/test.csv.gz", file.path(workdir,"test.csv.gz"), "wget", quiet = T)
train_hex <- h2o.importFile(h2oServer, path = file.path(workdir,"train.csv.gz"), header = F, sep = ',', key = 'train.hex')
test_hex <- h2o.importFile(h2oServer, path = file.path(workdir,"test.csv.gz"), header = F, sep = ',', key = 'test.hex')
```


### Performance Metrics and Observables

In this first part of the benchmark study, we will investigate the single-node performance of H2O Deep Learning in terms of training examples per second.

Notes:
  * For each set of `parameters` (in addition to specifying the training data, and the predictors and response column), we run H2O Deep Learning and record `training samples`, `training time`, `training speed`, and `test set error`.
  * H2O Deep Learning uses online learning with stochastic gradient descent and back-propagation. There are *no mini-batches*, every training sample immediately affects the network weights for fastest convergence and highest model accuracy.
  * [Hogwild!](http://papers.nips.cc/paper/4390-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent) intentional race conditions between multiple threads avoid locking, but leads to non-reproducible results, unless `reproducible=T` is specified (which then runs on 1 core - not done here).
  * In part 2 (below), we look at the test set error as a function of total training time, which is what ultimately matters most.
  * The clock starts at the moment the first training example is presented to the neural network, and stops when model building is done. The stopping criterion is the number of epochs (i.e., the number of training samples relative to the training data set size).
  * Hardware specs: [Intel i7 5820k (6 cores @4.3GHz)](http://rog.asus.com/realbench/show_comment.php?id=6632). Yes, it's an overclocked sub-$400 processor. Wish me luck.
  * The entire script ran in 12 minutes.  

Here we go. First, we set up some helper function to run H2O Deep Learning with user-given parameters, and to write the parameters and observables to CSV files.

```{r}
score_test_set=T  #disable if only interested in training throughput

run <- function(extra_params) {
  str(extra_params)
  print("Training.")
  model <- do.call(h2o.deeplearning, modifyList(list(x=1:784, y=785, do_classification=T, training_frame=train_hex, destination_key="dlmodel"), extra_params))
  samples <- (model@model$scoringHistory$"Training Samples")[length(model@model$scoringHistory$"Training Samples")]
  time <- model@model$run_time/1000
  print(paste0("training samples: ", samples))
  print(paste0("training time   : ", time, " seconds"))
  print(paste0("training speed  : ", samples/time, " samples/second"))
  
  if (score_test_set) {
    print("Scoring on test set.")
    ## Note: This scores full test set (10,000 rows) - can take time!
    test_error <- h2o.performance(model, test_hex)@metrics$cm$prediction_error
    print(paste0("test set error  : ", test_error))
  } else {
    test_error <- 1.0
  }
  h2o.rm("dlmodel")
  c(paste(names(extra_params), extra_params, sep = "=", collapse=" "), 
    samples, sprintf("%.3f", time), sprintf("%.3f", samples/time), sprintf("%.3f", test_error))
}

writecsv <- function(results, file) {
  table <- matrix(unlist(results), ncol = 5, byrow = TRUE)
  colnames(table) <- c("parameters", "training samples", "training time", "training speed", "test set error")
  write.csv(table, file.path(workdir,file), col.names = T, row.names=F, quote=T, sep=",")
}
```

### First Study - Various network topologies
As a first study, we vary the network topology, and we also fix a number of training samples, specified by the number of epochs (we process approximately 10% of the dataset here, chosen at random). All other parameters are left at default values (per-weight adaptive learning rate, no L1/L2 regularization, no Dropout, Rectifier activation function). Note that we run shallow neural nets (1 hidden layer), and deep neural nets with 2 or 3 hidden layers.  You can modify this and run other parameter combinations as well.

The number of columns of the dataset (all numerical) translates directly into the size of the first layer of neurons (input layer), and hence significantly affects the size of the model. The number of output neurons is 10, one for each class (digit) probability. For the MNIST dataset, all Deep Learning models below will have 717 input neurons, as the other 67 pixel values are constant (white background) and thus ignored. The size of the model is given by the number of connections between the fully connected neuron layers (and some bias values), and is hence roughly quadratic in the number of neurons per layer. The model complexity is linear in the size of the model, up to memory hierarchy effects in the x86 hardware.

```{r}
EPOCHS=0.1 #increase if you have the patience (or run multi-node), shouldn't matter much for single-node

args <- list(
  list(hidden=c(64),             epochs=EPOCHS),
  list(hidden=c(128),            epochs=EPOCHS),
  list(hidden=c(256),            epochs=EPOCHS),
  list(hidden=c(512),            epochs=EPOCHS),
  list(hidden=c(1024),           epochs=EPOCHS),
  list(hidden=c(64,64),          epochs=EPOCHS),
  list(hidden=c(128,128),        epochs=EPOCHS),
  list(hidden=c(256,256),        epochs=EPOCHS),
  list(hidden=c(512,512),        epochs=EPOCHS),
  list(hidden=c(1024,1024),      epochs=EPOCHS),
  list(hidden=c(64,64,64),       epochs=EPOCHS),
  list(hidden=c(128,128,128),    epochs=EPOCHS),
  list(hidden=c(256,256,256),    epochs=EPOCHS),
  list(hidden=c(512,512,512),    epochs=EPOCHS),
  list(hidden=c(1024,1024,1024), epochs=EPOCHS)
)
writecsv(lapply(args, run), "network_topology.csv")
```
We can plot the training speed (x-axis, more is faster) for the runs above, in the same order as the listing above. On the y-axis, we denote the overall training time in seconds.

<div id='plot_network_topology'></div>

As expected, smaller and shallower networks run faster, and larger, deeper networks take more time. An interesting exception is the smallest network (717x64x10 neurons), which has only `717*64 + 64*10 + 64 + 10 = 46602` model parameters (plus adaptive learning rate overhead of a factor of 2), and some overhead (not clear whether it's hardware or software, need to look into it) prevents this from running faster. If you are interested, you can confirm the network topology and model size (among other useful things) by inspecting the logs (available via the Flow GUI on [localhost:54321](http://localhost:54321)):

```{r}
# INFO: Number of model parameters (weights/biases): 46,602
# ...
# INFO: Number of hidden layers is 1 
# INFO: Status of Neuron Layers:
# INFO:  # Units      Type Dropout     L1     L2   Rate (Mean,RMS)  Weight (Mean,RMS)  Bias (Mean,RMS)
# INFO:  1   717     Input  0.00 %                                                                              
# INFO:  2    64 Rectifier  0.00 % 0.0000 0.0000  (0.0000, 0.0000)  (0.0000, 0.0000)  (0.0000, 0.0000)
# INFO:  3    10   Softmax         0.0000 0.0000  (0.0000, 0.0000)  (0.0000, 0.0000)  (0.0000, 0.0000)
```
For your convenience, here's the raw data from the CSV file `network_topology.csv` in directory `work_dir`:
<div id='table_network_topology'></div>

### Scoring overhead

```{r}
args <- list(
  list(hidden=c(1024, 1024), epochs=EPOCHS, validation_frame=test_hex),
  list(hidden=c(1024, 1024), epochs=EPOCHS, score_training_samples=60000, score_duty_cycle=1, score_interval=1),
  list(hidden=c(1024, 1024), epochs=EPOCHS, score_training_samples=60000),
  list(hidden=c(1024, 1024), epochs=EPOCHS, score_training_samples=10000),
  list(hidden=c(1024, 1024), epochs=EPOCHS, score_training_samples=10000),
  list(hidden=c(1024, 1024), epochs=EPOCHS, score_training_samples=1000),
  list(hidden=c(1024, 1024), epochs=EPOCHS, score_training_samples=100),
  list(hidden=c(1024, 1024), epochs=EPOCHS, score_training_samples=100, score_duty_cycle=0, score_interval=10000)
)
writecsv(lapply(args, run), "scoring_overhead.csv")
```
<div id='plot_scoring_overhead'></div>
<div id='table_scoring_overhead'></div>


```{r}
## Adaptive (per-coefficient) learning rate vs manual learning rate and momentum
args <- list(
  list(hidden=c(1024, 1024), epochs=EPOCHS, score_training_samples=100, adaptive_rate=T),
  list(hidden=c(1024, 1024), epochs=EPOCHS, score_training_samples=100, adaptive_rate=T, rho=0.95, epsilon=1e-6),
  list(hidden=c(1024, 1024), epochs=EPOCHS, score_training_samples=100, adaptive_rate=F),
  list(hidden=c(1024, 1024), epochs=EPOCHS, score_training_samples=100, adaptive_rate=F, rate=1e-3, momentum_start=0.5, momentum_ramp=1e5, momentum_stable=0.99)
)
writecsv(lapply(args, run), "adaptive_rate.csv")
```
<div id='plot_adaptive_rate'></div>
<div id='table_adaptive_rate'></div>




```{r}
## Modify duration of a MapReduce step in terms of train_samples_per_iteration
args <- list(
  #auto-tuning
  list(hidden=c(1024, 1024), epochs=EPOCHS, score_training_samples=100, train_samples_per_iteration=-2),
  #lots of comm. overhead 
  list(hidden=c(1024, 1024), epochs=EPOCHS, score_training_samples=100, train_samples_per_iteration=100),  
  #reasonable comm. overhead
  list(hidden=c(1024, 1024), epochs=EPOCHS, score_training_samples=100, train_samples_per_iteration=1000), 
  #little comm. overhead
  list(hidden=c(1024, 1024), epochs=EPOCHS, score_training_samples=100, train_samples_per_iteration=6000)  
)
writecsv(lapply(args, run), "train_samples_per_iteration.csv")
```
<div id='plot_train_samples_per_iteration'></div>
<div id='table_train_samples_per_iteration'></div>



```{r}
## Different activation functions
args <- list(
  list(hidden=c(1024, 1024), epochs=EPOCHS, score_training_samples=100, train_samples_per_iteration=1000, activation="Rectifier"),
  list(hidden=c(1024, 1024), epochs=EPOCHS, score_training_samples=100, train_samples_per_iteration=1000, activation="RectifierWithDropout"),
  list(hidden=c(1024, 1024), epochs=EPOCHS, score_training_samples=100, train_samples_per_iteration=1000, activation="Tanh"),
  list(hidden=c(1024, 1024), epochs=EPOCHS, score_training_samples=100, train_samples_per_iteration=1000, activation="TanhWithDropout")
)
writecsv(lapply(args, run), "activation_function.csv")
```
<div id='plot_activation_function'></div>
<div id='table_activation_function'></div>




```{r}
## Large neural network - compare to external H2O Benchmark study (of throughput only - should consider (test set) accuracy as well)
## c.f. http://www.comp.nus.edu.sg/~dbsystem/singa/development/2015/01/29/compare-h2o/
#score_test_set=F
args <- list(
  ## parameters used by http://www.comp.nus.edu.sg/~dbsystem/singa/development/2015/01/29/compare-h2o/
  ## (scores on 10k training rows and full validation frame) and adaptive learning rate
  list(hidden=c(2500,2000,1500,1000,500), epochs=EPOCHS, train_samples_per_iteration=1500, activation="Tanh", validation_frame=test_hex),                  
  
  ## optimized for throughput - remove validation frame, reduce training set scoring sample, disable adaptive_rate (seems to help initial convergence)
  list(hidden=c(2500,2000,1500,1000,500), epochs=EPOCHS, train_samples_per_iteration=1500, activation="Tanh", adaptive_rate=F, score_training_samples=100, score_duty_cycle=0),
  
  ## Rectifier with Dropout - even faster
  list(hidden=c(2500,2000,1500,1000,500), epochs=EPOCHS, train_samples_per_iteration=1500, activation="RectifierWithDropout", adaptive_rate=F, score_training_samples=100, score_duty_cycle=0)
)
writecsv(lapply(args, run), "large_deep_net.csv")
```
<div id='plot_large_deep_net'></div>
<div id='table_large_deep_net'></div>




## Part 2 - What Really Matters: Test Set Error vs Training Time
### Use above benchmark results to select good candidates for parameters

```{r}
EPOCHS=2
args <- list(
  list(epochs=EPOCHS),
  list(epochs=EPOCHS,   activation="Tanh"),
  list(epochs=EPOCHS,   hidden=c(200,200), activation="RectifierWithDropout", input_dropout_ratio=0.2, l1=1e-5),
  list(epochs=EPOCHS,   hidden=c(100,100,100)),
  list(epochs=EPOCHS,   hidden=c(100,100,100), activation="RectifierWithDropout", input_dropout_ratio=0.2, l1=1e-5),
  list(epochs=EPOCHS,   hidden=c(100,100,100,100), activation="RectifierWithDropout", input_dropout_ratio=0.2, l1=1e-5),
  list(epochs=5*EPOCHS, hidden=c(200,200), activation="RectifierWithDropout", input_dropout_ratio=0.2, l1=1e-5),
  list(epochs=5*EPOCHS, hidden=c(100,100,100)),
  list(epochs=5*EPOCHS, hidden=c(100,100,100), activation="RectifierWithDropout", input_dropout_ratio=0.2, l1=1e-5),
  list(epochs=5*EPOCHS, hidden=c(100,100,100,100), activation="RectifierWithDropout", input_dropout_ratio=0.2, l1=1e-5)
)
writecsv(lapply(args, run), "what_really_matters.csv")
```
<div id='plot_what_really_matters'></div>
<div id='table_what_really_matters'></div>

### Summary from all models built so far
Here's a plot summarizing all the models we've built so far.

<div id='plot_all'></div>
<div id='table_all'></div>


### World-record Test Set Error (without Convolutions or Pre-Training)
With the simple 1-line command in R shown below, we achieve 0.83% test set error, which is the current world record for models without convolutional layers, data augmentation or unsupervised pre-training. This takes about 10 hours on a 10-node cluster with dual 8-core Xeons. Note that this model achieves 1% test set error in about 2 hours, and 0.9% test set error in about 4 hours, so it's clear that improving the regularization properties of the model can take a long time (and is clearly in the statistical noise domain with only 10k test set points).

    #   > record_model <- h2o.deeplearning(x = 1:784, y = 785, do_classification=T, training_frame=train_hex, validation_frame = test_hex,
    #                                      activation = "RectifierWithDropout", hidden = c(1024,1024,2048),
    #                                      epochs = 8000, l1 = 1e-5, input_dropout_ratio = 0.2,
    #                                      train_samples_per_iteration = -1, classification_stop = -1)
    #   > record_model@model$validMetrics$cm$prediction_error
    #              
    #     Act/Pred   0    1    2    3   4   5   6    7   8    9 Error
    #       0      974    1    1    0   0   0   2    1   1    0 0.00612
    #       1        0 1135    0    1   0   0   0    0   0    0 0.00088
    #       2        0    0 1028    0   1   0   0    3   0    0 0.00388
    #       3        0    0    1 1003   0   0   0    3   2    1 0.00693
    #       4        0    0    1    0 971   0   4    0   0    6 0.01120
    #       5        2    0    0    5   0 882   1    1   1    0 0.01121
    #       6        2    3    0    1   1   2 949    0   0    0 0.00939
    #       7        1    2    6    0   0   0   0 1019   0    0 0.00875
    #       8        1    0    1    3   0   4   0    2 960    3 0.01437
    #       9        1    2    0    0   4   3   0    2   0  997 0.01189
    #       Totals 981 1142 1038 1013 977 891 956 1031 964 1007 0.00830

## Part 3 - Multi-Node Benchmarks Coming Soon!

<!--TODO: don't count final scoring towards training time, set score_interval high and avoid training set scoring-->
<!--NOTE: score_training_sample might not be enforceable on many nodes-->
