{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This tutorial shows how a H2O [Generalized Linear Model](https://en.wikipedia.org/wiki/Generalized_linear_model) model can be used to do supervised classification. This tutorial covers usage of H2O from Python. An R version of this tutorial will be available as well in a separate document. This file is available in plain R, plain Python and iPython Notebook formats. More examples and explanations can be found in our [H2O Generalized Linear Modeling booklet](http://h2o.ai/resources/) and on our [H2O Github Repository](http://github.com/h2oai/h2o-3/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H2O Python Module\n",
    "\n",
    "Load the H2O Python module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h2o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start H2O\n",
    "Start up a 1-node H2O cloud on your local machine, and allow it to use all CPU cores and up to 2GB of memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime: </td>\n",
       "<td>57 minutes 1 seconds 333 milliseconds </td></tr>\n",
       "<tr><td>H2O cluster version: </td>\n",
       "<td>3.7.0.3248</td></tr>\n",
       "<tr><td>H2O cluster name: </td>\n",
       "<td>H2O_started_from_python</td></tr>\n",
       "<tr><td>H2O cluster total nodes: </td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster total memory: </td>\n",
       "<td>1.78 GB</td></tr>\n",
       "<tr><td>H2O cluster total cores: </td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster allowed cores: </td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster healthy: </td>\n",
       "<td>True</td></tr>\n",
       "<tr><td>H2O Connection ip: </td>\n",
       "<td>127.0.0.1</td></tr>\n",
       "<tr><td>H2O Connection port: </td>\n",
       "<td>54321</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  -------------------------------------\n",
       "H2O cluster uptime:         57 minutes 1 seconds 333 milliseconds\n",
       "H2O cluster version:        3.7.0.3248\n",
       "H2O cluster name:           H2O_started_from_python\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster total memory:   1.78 GB\n",
       "H2O cluster total cores:    8\n",
       "H2O cluster allowed cores:  8\n",
       "H2O cluster healthy:        True\n",
       "H2O Connection ip:          127.0.0.1\n",
       "H2O Connection port:        54321\n",
       "--------------------------  -------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h2o.init(max_mem_size_GB = 2)            #uses all cores by default\n",
    "h2o.remove_all()                          #clean slate, in case cluster was already running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more about the h2o package itself, we can use Python's builtin help() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(h2o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "help() can be used on H2O functions and models. Jupyter's builtin shift-tab functionality also works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class H2OGeneralizedLinearEstimator in module h2o.estimators.glm:\n",
      "\n",
      "class H2OGeneralizedLinearEstimator(h2o.estimators.estimator_base.H2OEstimator)\n",
      " |  Method resolution order:\n",
      " |      H2OGeneralizedLinearEstimator\n",
      " |      h2o.estimators.estimator_base.H2OEstimator\n",
      " |      h2o.model.model_base.ModelBase\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, model_id=None, max_iterations=None, beta_epsilon=None, solver=None, standardize=None, family=None, link=None, tweedie_variance_power=None, tweedie_link_power=None, alpha=None, prior=None, lambda_search=None, nlambdas=None, lambda_min_ratio=None, beta_constraints=None, nfolds=None, fold_assignment=None, keep_cross_validation_predictions=None, intercept=None, Lambda=None, max_active_predictors=None, checkpoint=None)\n",
      " |      Build a Generalized Linear Model\n",
      " |      Fit a generalized linear model, specified by a response variable, a set of predictors,\n",
      " |      and a description of the error distribution.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      model_id : str, optional\n",
      " |        The unique id assigned to the resulting model. If none is given, an id will\n",
      " |        automatically be generated.\n",
      " |      max_iterations : int\n",
      " |        A non-negative integer specifying the maximum number of iterations.\n",
      " |      beta_epsilon : int\n",
      " |        A non-negative number specifying the magnitude of the maximum difference between\n",
      " |        the coefficient estimates from successive iterations. Defines the convergence\n",
      " |        criterion.\n",
      " |      solver : str\n",
      " |        A character string specifying the solver used: IRLSM (supports more features),\n",
      " |        L_BFGS (scales better for datasets with many columns)\n",
      " |      standardize : bool\n",
      " |        Indicates whether the numeric predictors should be standardized to have a mean of\n",
      " |        0 and a variance of 1 prior to training the models.\n",
      " |      family : str\n",
      " |        A character string specifying the distribution of the model:\n",
      " |       gaussian, binomial, poisson, gamma, tweedie.\n",
      " |      link : str\n",
      " |        A character string specifying the link function.\n",
      " |        The default is the canonical link for the family.\n",
      " |        The supported links for each of the family specifications are:\n",
      " |            \"gaussian\": \"identity\", \"log\", \"inverse\"\n",
      " |            \"binomial\": \"logit\", \"log\"\n",
      " |            \"poisson\": \"log\", \"identity\"\n",
      " |            \"gamma\": \"inverse\", \"log\", \"identity\"\n",
      " |            \"tweedie\": \"tweedie\"\n",
      " |      \n",
      " |      tweedie_variance_power : int\n",
      " |        numeric specifying the power for the variance function when family = \"tweedie\".\n",
      " |      tweedie_link_power : int\n",
      " |        A numeric specifying the power for the link function when family = \"tweedie\".\n",
      " |      alpha : float\n",
      " |        A numeric in [0, 1] specifying the elastic-net mixing parameter.\n",
      " |      \n",
      " |        The elastic-net penalty is defined to be:\n",
      " |        eqn{P(\u0007lphaeta) = (1-\u0007lpha)/2|eta||_2^2 +\n",
      " |        \u0007lpha|eta||_1 = \\sum_j [(1-\u0007lpha)/2eta_j^2 + \u0007lphaeta_j|],\n",
      " |        making alpha = 1 the lasso penalty and alpha = 0 the ridge penalty.\n",
      " |      \n",
      " |      Lambda : float\n",
      " |        A non-negative shrinkage parameter for the elastic-net, which multiplies\n",
      " |        \\eqn{P(\u0007lphaeta) in the objective function.\n",
      " |        When Lambda = 0, no elastic-net penalty is applied and ordinary generalized linear\n",
      " |        models are fit.\n",
      " |      prior : float, optional\n",
      " |        A numeric specifying the prior probability of class 1 in the response when\n",
      " |        family = \"binomial\". The default prior is the observational frequency of class 1.\n",
      " |      lambda_search : bool\n",
      " |        A logical value indicating whether to conduct a search over the space of lambda\n",
      " |        values starting from the lambda max, given lambda is interpreted as lambda minself.\n",
      " |      nlambdas : int\n",
      " |        The number of lambda values to use when lambda_search = TRUE.\n",
      " |      lambda_min_ratio : float\n",
      " |        Smallest value for lambda as a fraction of lambda.max. By default if the number of\n",
      " |        observations is greater than the the number of variables then\n",
      " |        lambda_min_ratio = 0.0001; if the number of observations is less than the number\n",
      " |        of variables then lambda_min_ratio = 0.01.\n",
      " |      beta_constraints : H2OFrame\n",
      " |        A data.frame or H2OParsedData object with the columns\n",
      " |        [\"names\", \"lower_bounds\", \"upper_bounds\", \"beta_given\"],\n",
      " |        where each row corresponds to a predictor in the GLM.\n",
      " |        \"names\" contains the predictor names, \"lower\"/\"upper_bounds\",\n",
      " |        are the lower and upper bounds of beta, and \"beta_given\" is some supplied starting\n",
      " |        values.\n",
      " |      nfolds : int, optional\n",
      " |        Number of folds for cross-validation. If nfolds >= 2, then validation must\n",
      " |        remain empty.\n",
      " |      fold_assignment : str\n",
      " |        Cross-validation fold assignment scheme, if fold_column is not specified Must be\n",
      " |        \"AUTO\", \"Random\" or \"Modulo\"\n",
      " |      keep_cross_validation_predictions : bool\n",
      " |        Whether to keep the predictions of the cross-validation models\n",
      " |      intercept : bool\n",
      " |        Logical, include constant term (intercept) in the model\n",
      " |      max_active_predictors : int, optional\n",
      " |        Convergence criteria for number of predictors when using L1 penalty.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |        A subclass of ModelBase is returned. The specific subclass depends on the machine\n",
      " |        learning task at hand (if it's binomial classification, then an H2OBinomialModel\n",
      " |        is returned, if it's regression then a H2ORegressionModel is returned). The default\n",
      " |        print-out of the models is shown, but further GLM-specifc information can be\n",
      " |        queried out of the object. Upon completion of the GLM, the resulting object has\n",
      " |        coefficients, normalized coefficients, residual/null deviance, aic, and a host of\n",
      " |        model metrics including MSE, AUC (for logistic regression), degrees of freedom, and\n",
      " |        confusion matrices.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  Lambda\n",
      " |  \n",
      " |  alpha\n",
      " |  \n",
      " |  beta_constraints\n",
      " |  \n",
      " |  beta_epsilon\n",
      " |  \n",
      " |  checkpoint\n",
      " |  \n",
      " |  family\n",
      " |  \n",
      " |  fold_assignment\n",
      " |  \n",
      " |  intercept\n",
      " |  \n",
      " |  keep_cross_validation_predictions\n",
      " |  \n",
      " |  lambda_min_ratio\n",
      " |  \n",
      " |  lambda_search\n",
      " |  \n",
      " |  link\n",
      " |  \n",
      " |  max_active_predictors\n",
      " |  \n",
      " |  max_iterations\n",
      " |  \n",
      " |  nfolds\n",
      " |  \n",
      " |  nlambdas\n",
      " |  \n",
      " |  prior\n",
      " |  \n",
      " |  solver\n",
      " |  \n",
      " |  standardize\n",
      " |  \n",
      " |  tweedie_link_power\n",
      " |  \n",
      " |  tweedie_variance_power\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from h2o.estimators.estimator_base.H2OEstimator:\n",
      " |  \n",
      " |  build_model(self, algo_params)\n",
      " |  \n",
      " |  fit(self, X, y=None, **params)\n",
      " |      Fit an H2O model as part of a scikit-learn pipeline or grid search.\n",
      " |      \n",
      " |      A warning will be issued if a caller other than sklearn attempts to use this method.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |        X : H2OFrame\n",
      " |          An H2OFrame consisting of the predictor variables.\n",
      " |        y : H2OFrame, optional\n",
      " |          An H2OFrame consisting of the response variable.\n",
      " |        params : optional\n",
      " |          Extra arguments.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |        The current instance of H2OEstimator for method chaining.\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Useful method for obtaining parameters for this estimator. Used primarily for\n",
      " |      sklearn Pipelines and sklearn grid search.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |        deep : bool, optional\n",
      " |          If True, return parameters of all sub-objects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |        A dict of parameters\n",
      " |  \n",
      " |  join(self)\n",
      " |  \n",
      " |  set_params(self, **parms)\n",
      " |      Used by sklearn for updating parameters during grid search.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |        parms : dict\n",
      " |          A dictionary of parameters that will be set on this model.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |        Returns self, the current estimator object with the parameters all set as desired.\n",
      " |  \n",
      " |  start(self, x, y=None, training_frame=None, offset_column=None, fold_column=None, weights_column=None, validation_frame=None, **params)\n",
      " |      Asynchronous model build by specifying the predictor columns, response column, and any\n",
      " |      additional frame-specific values.\n",
      " |      \n",
      " |      To block for results, call join.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |        x : list\n",
      " |          A list of column names or indices indicating the predictor columns.\n",
      " |        y : str\n",
      " |          An index or a column name indicating the response column.\n",
      " |        training_frame : H2OFrame\n",
      " |          The H2OFrame having the columns indicated by x and y (as well as any\n",
      " |          additional columns specified by fold, offset, and weights).\n",
      " |        offset_column : str, optional\n",
      " |          The name or index of the column in training_frame that holds the offsets.\n",
      " |        fold_column : str, optional\n",
      " |          The name or index of the column in training_frame that holds the per-row fold\n",
      " |          assignments.\n",
      " |        weights_column : str, optional\n",
      " |          The name or index of the column in training_frame that holds the per-row weights.\n",
      " |        validation_frame : H2OFrame, optional\n",
      " |          H2OFrame with validation data to be scored on while training.\n",
      " |  \n",
      " |  train(self, x, y=None, training_frame=None, offset_column=None, fold_column=None, weights_column=None, validation_frame=None, **params)\n",
      " |      Train the H2O model by specifying the predictor columns, response column, and any\n",
      " |      additional frame-specific values.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |        x : list\n",
      " |          A list of column names or indices indicating the predictor columns.\n",
      " |        y : str\n",
      " |          An index or a column name indicating the response column.\n",
      " |        training_frame : H2OFrame\n",
      " |          The H2OFrame having the columns indicated by x and y (as well as any\n",
      " |          additional columns specified by fold, offset, and weights).\n",
      " |        offset_column : str, optional\n",
      " |          The name or index of the column in training_frame that holds the offsets.\n",
      " |        fold_column : str, optional\n",
      " |          The name or index of the column in training_frame that holds the per-row fold\n",
      " |          assignments.\n",
      " |        weights_column : str, optional\n",
      " |          The name or index of the column in training_frame that holds the per-row weights.\n",
      " |        validation_frame : H2OFrame, optional\n",
      " |          H2OFrame with validation data to be scored on while training.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from h2o.estimators.estimator_base.H2OEstimator:\n",
      " |  \n",
      " |  mixin(obj, cls)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from h2o.model.model_base.ModelBase:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  aic(self, train=False, valid=False, xval=False)\n",
      " |      Get the AIC(s).\n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\", \"valid\",\n",
      " |      and \"xval\"\n",
      " |      \n",
      " |      :param train: If train is True, then return the AIC value for the training data.\n",
      " |      :param valid: If valid is True, then return the AIC value for the validation data.\n",
      " |      :param xval:  If xval is True, then return the AIC value for the validation data.\n",
      " |      :return: The AIC.\n",
      " |  \n",
      " |  auc(self, train=False, valid=False, xval=False)\n",
      " |      Get the AUC(s).\n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\", \"valid\",\n",
      " |      and \"xval\"\n",
      " |      \n",
      " |      :param train: If train is True, then return the AUC value for the training data.\n",
      " |      :param valid: If valid is True, then return the AUC value for the validation data.\n",
      " |      :param xval:  If xval is True, then return the AUC value for the validation data.\n",
      " |      :return: The AUC.\n",
      " |  \n",
      " |  biases(self, vector_id=0)\n",
      " |      Return the frame for the respective bias vector\n",
      " |      :param: vector_id: an integer, ranging from 0 to number of layers, that specifies the bias vector to return.\n",
      " |      :return: an H2OFrame which represents the bias vector identified by vector_id\n",
      " |  \n",
      " |  catoffsets(self)\n",
      " |      Categorical offsets for one-hot encoding\n",
      " |  \n",
      " |  coef(self)\n",
      " |      :return: Return the coefficients for this model.\n",
      " |  \n",
      " |  coef_norm(self)\n",
      " |      :return: Return the normalized coefficients\n",
      " |  \n",
      " |  deepfeatures(self, test_data, layer)\n",
      " |      Return hidden layer details\n",
      " |      \n",
      " |      :param test_data: Data to create a feature space on\n",
      " |      :param layer: 0 index hidden layer\n",
      " |  \n",
      " |  download_pojo(self, path='')\n",
      " |      Download the POJO for this model to the directory specified by path (no trailing slash!).\n",
      " |      If path is \"\", then dump to screen.\n",
      " |      :param model: Retrieve this model's scoring POJO.\n",
      " |      :param path:  An absolute path to the directory where POJO should be saved.\n",
      " |      :return: None\n",
      " |  \n",
      " |  get_xval_models(self, key=None)\n",
      " |      Return a Model object.\n",
      " |      \n",
      " |      :param key: If None, return all cross-validated models; otherwise return the model that key points to.\n",
      " |      :return: A model or list of models.\n",
      " |  \n",
      " |  giniCoef(self, train=False, valid=False, xval=False)\n",
      " |      Get the Gini Coefficient(s).\n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\", \"valid\",\n",
      " |      and \"xval\"\n",
      " |      \n",
      " |      :param train: If train is True, then return the Gini Coefficient value for the training data.\n",
      " |      :param valid: If valid is True, then return the Gini Coefficient value for the validation data.\n",
      " |      :param xval:  If xval is True, then return the Gini Coefficient value for the cross validation data.\n",
      " |      :return: The Gini Coefficient for this binomial model.\n",
      " |  \n",
      " |  is_cross_validated(self)\n",
      " |      :return:  True if the model was cross-validated.\n",
      " |  \n",
      " |  logloss(self, train=False, valid=False, xval=False)\n",
      " |      Get the Log Loss(s).\n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\", \"valid\",\n",
      " |      and \"xval\"\n",
      " |      \n",
      " |      :param train: If train is True, then return the Log Loss value for the training data.\n",
      " |      :param valid: If valid is True, then return the Log Loss value for the validation data.\n",
      " |      :param xval:  If xval is True, then return the Log Loss value for the cross validation data.\n",
      " |      :return: The Log Loss for this binomial model.\n",
      " |  \n",
      " |  mean_residual_deviance(self, train=False, valid=False, xval=False)\n",
      " |      Get the Mean Residual Deviances(s).\n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\", \"valid\",\n",
      " |      and \"xval\"\n",
      " |      \n",
      " |      :param train: If train is True, then return the Mean Residual Deviance value for the training data.\n",
      " |      :param valid: If valid is True, then return the Mean Residual Deviance value for the validation data.\n",
      " |      :param xval:  If xval is True, then return the Mean Residual Deviance value for the cross validation data.\n",
      " |      :return: The Mean Residual Deviance for this regression model.\n",
      " |  \n",
      " |  model_performance(self, test_data=None, train=False, valid=False)\n",
      " |      Generate model metrics for this model on test_data.\n",
      " |      \n",
      " |      :param test_data: Data set for which model metrics shall be computed against. Both train and valid arguments are ignored if test_data is not None.\n",
      " |      :param train: Report the training metrics for the model. If the test_data is the training data, the training metrics are returned.\n",
      " |      :param valid: Report the validation metrics for the model. If train and valid are True, then it defaults to True.\n",
      " |      :return: An object of class H2OModelMetrics.\n",
      " |  \n",
      " |  mse(self, train=False, valid=False, xval=False)\n",
      " |      Get the MSE(s).\n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\", \"valid\",\n",
      " |      and \"xval\"\n",
      " |      \n",
      " |      :param train: If train is True, then return the MSE value for the training data.\n",
      " |      :param valid: If valid is True, then return the MSE value for the validation data.\n",
      " |      :param xval:  If xval is True, then return the MSE value for the cross validation data.\n",
      " |      :return: The MSE for this regression model.\n",
      " |  \n",
      " |  normmul(self)\n",
      " |      Normalization/Standardization multipliers for numeric predictors\n",
      " |  \n",
      " |  normsub(self)\n",
      " |      Normalization/Standardization offsets for numeric predictors\n",
      " |  \n",
      " |  null_degrees_of_freedom(self, train=False, valid=False, xval=False)\n",
      " |      Retreive the null degress of freedom if this model has the attribute, or None otherwise.\n",
      " |      \n",
      " |      :param train: Get the null dof for the training set. If both train and valid are False, then train is selected by default.\n",
      " |      :param valid: Get the null dof for the validation set. If both train and valid are True, then train is selected by default.\n",
      " |      :return: Return the null dof, or None if it is not present.\n",
      " |  \n",
      " |  null_deviance(self, train=False, valid=False, xval=False)\n",
      " |      Retreive the null deviance if this model has the attribute, or None otherwise.\n",
      " |      \n",
      " |      :param:  train Get the null deviance for the training set. If both train and valid are False, then train is selected by default.\n",
      " |      :param:  valid Get the null deviance for the validation set. If both train and valid are True, then train is selected by default.\n",
      " |      :return: Return the null deviance, or None if it is not present.\n",
      " |  \n",
      " |  pprint_coef(self)\n",
      " |      Pretty print the coefficents table (includes normalized coefficients)\n",
      " |      :return: None\n",
      " |  \n",
      " |  predict(self, test_data)\n",
      " |      Predict on a dataset.\n",
      " |      \n",
      " |      :param test_data: Data to be predicted on.\n",
      " |      :return: A new H2OFrame filled with predictions.\n",
      " |  \n",
      " |  r2(self, train=False, valid=False, xval=False)\n",
      " |      Return the R^2 for this regression model.\n",
      " |      \n",
      " |      The R^2 value is defined to be 1 - MSE/var,\n",
      " |      where var is computed as sigma*sigma.\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\", \"valid\",\n",
      " |      and \"xval\"\n",
      " |      \n",
      " |      :param train: If train is True, then return the R^2 value for the training data.\n",
      " |      :param valid: If valid is True, then return the R^2 value for the validation data.\n",
      " |      :param xval:  If xval is True, then return the R^2 value for the cross validation data.\n",
      " |      :return: The R^2 for this regression model.\n",
      " |  \n",
      " |  residual_degrees_of_freedom(self, train=False, valid=False, xval=False)\n",
      " |      Retreive the residual degress of freedom if this model has the attribute, or None otherwise.\n",
      " |      \n",
      " |      :param train: Get the residual dof for the training set. If both train and valid are False, then train is selected by default.\n",
      " |      :param valid: Get the residual dof for the validation set. If both train and valid are True, then train is selected by default.\n",
      " |      :return: Return the residual dof, or None if it is not present.\n",
      " |  \n",
      " |  residual_deviance(self, train=False, valid=False, xval=False)\n",
      " |      Retreive the residual deviance if this model has the attribute, or None otherwise.\n",
      " |      \n",
      " |      :param train: Get the residual deviance for the training set. If both train and valid are False, then train is selected by default.\n",
      " |      :param valid: Get the residual deviance for the validation set. If both train and valid are True, then train is selected by default.\n",
      " |      :return: Return the residual deviance, or None if it is not present.\n",
      " |  \n",
      " |  respmul(self)\n",
      " |      Normalization/Standardization multipliers for numeric response\n",
      " |  \n",
      " |  respsub(self)\n",
      " |      Normalization/Standardization offsets for numeric response\n",
      " |  \n",
      " |  score_history(self)\n",
      " |      Retrieve Model Score History\n",
      " |      :return: the score history (H2OTwoDimTable)\n",
      " |  \n",
      " |  show(self)\n",
      " |      Print innards of model, without regards to type\n",
      " |      \n",
      " |      :return: None\n",
      " |  \n",
      " |  summary(self)\n",
      " |      Print a detailed summary of the model.\n",
      " |      \n",
      " |      :return:\n",
      " |  \n",
      " |  varimp(self, return_list=False)\n",
      " |      Pretty print the variable importances, or return them in a list\n",
      " |      :param return_list: if True, then return the variable importances in an list (ordered from most important to least\n",
      " |      important). Each entry in the list is a 4-tuple of (variable, relative_importance, scaled_importance, percentage).\n",
      " |      :return: None or ordered list\n",
      " |  \n",
      " |  weights(self, matrix_id=0)\n",
      " |      Return the frame for the respective weight matrix\n",
      " |      :param: matrix_id: an integer, ranging from 0 to number of layers, that specifies the weight matrix to return.\n",
      " |      :return: an H2OFrame which represents the weight matrix identified by matrix_id\n",
      " |  \n",
      " |  xval_keys(self)\n",
      " |      :return: The model keys for the cross-validated model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from h2o.model.model_base.ModelBase:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  full_parameters\n",
      " |      Get the full specification of all parameters.\n",
      " |      \n",
      " |      :return: a dictionary of parameters used to build this model.\n",
      " |  \n",
      " |  model_id\n",
      " |      :return: Retrieve this model's identifier.\n",
      " |  \n",
      " |  params\n",
      " |      Get the parameters and the actual/default values only.\n",
      " |      \n",
      " |      :return: A dictionary of parameters used to build this model.\n",
      " |  \n",
      " |  xvals\n",
      " |      Return a list of the cross-validated models.\n",
      " |      \n",
      " |      :return: A list of models\n",
      "\n",
      "Help on function import_file in module h2o.h2o:\n",
      "\n",
      "import_file(path=None, destination_frame='', parse=True, header=(-1, 0, 1), sep='', col_names=None, col_types=None, na_strings=None)\n",
      "    Have H2O import a dataset into memory. The path to the data must be a valid path for\n",
      "    each node in the H2O cluster. If some node in the H2O cluster cannot see the file, then\n",
      "    an exception will be thrown by the H2O cluster.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "      path : str\n",
      "        A path specifying the location of the data to import.\n",
      "      destination_frame : str, optional\n",
      "        The unique hex key assigned to the imported file. If none is given, a key will\n",
      "        automatically be generated.\n",
      "      parse : bool, optional\n",
      "        A logical value indicating whether the file should be parsed after import.\n",
      "      header : int, optional\n",
      "       -1 means the first line is data, 0 means guess, 1 means first line is header.\n",
      "      sep : str, optional\n",
      "        The field separator character. Values on each line of the file are separated by this\n",
      "        character. If sep = \"\", the parser will automatically detect the separator.\n",
      "      col_names : list, optional\n",
      "        A list of column names for the file.\n",
      "      col_types : list or dict, optional\n",
      "        A list of types or a dictionary of column names to types to specify whether columns\n",
      "        should be forced to a certain type upon import parsing. If a list, the types for\n",
      "        elements that are None will be guessed. The possible types a column may have are:\n",
      "            \"unknown\" - this will force the column to be parsed as all NA\n",
      "            \"uuid\"    - the values in the column must be true UUID or will be parsed as NA\n",
      "            \"string\"  - force the column to be parsed as a string\n",
      "            \"numeric\" - force the column to be parsed as numeric. H2O will handle the\n",
      "                        compression of the numeric data in the optimal manner.\n",
      "            \"enum\"    - force the column to be parsed as a categorical column.\n",
      "            \"time\"    - force the column to be parsed as a time column. H2O will attempt to\n",
      "                        parse the following list of date time formats.\n",
      "                          date:\n",
      "                            \"yyyy-MM-dd\"\n",
      "                            \"yyyy MM dd\"\n",
      "                            \"dd-MMM-yy\"\n",
      "                            \"dd MMM yy\"\n",
      "                          time:\n",
      "                            \"HH:mm:ss\"\n",
      "                            \"HH:mm:ss:SSS\"\n",
      "                            \"HH:mm:ss:SSSnnnnnn\"\n",
      "                            \"HH.mm.ss\"\n",
      "                            \"HH.mm.ss.SSS\"\n",
      "                            \"HH.mm.ss.SSSnnnnnn\"\n",
      "                        Times can also contain \"AM\" or \"PM\".\n",
      "      na_strings : list or dict, optional\n",
      "        A list of strings, or a list of lists of strings (one list per column), or a\n",
      "        dictionary of column names to strings which are to be interpreted as missing values.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "      A new H2OFrame instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n",
    "help(H2OGeneralizedLinearEstimator)\n",
    "help(h2o.import_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we use pandas DataFrames to simplify some processes later in this demo, let's import both pandas and numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##H2O GLM\n",
    "\n",
    "Generalized linear models (GLMs) are an extension of traditional linear models. They have gained popularity in statistical data analysis due to:  \n",
    "\n",
    "1. the flexibility of the model structure unifying the typical regression methods (such as linear regression and logistic regression for binary classification)  \n",
    "2. the recent availability of model-fitting software  \n",
    "3. the ability to scale well with large datasets  \n",
    "\n",
    "H2O's GLM algorithm fits generalized linear models to the data by maximizing the log-likelihood. The elastic net penalty can be used for parameter regularization. The model fitting computation is distributed, extremely fast, and scales extremely well for models with a limited number of predictors with non-zero coefficients (~ low thousands).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Getting started\n",
    "\n",
    "We begin by importing our data into H2OFrames, which operate similarly in function to pandas DataFrames but exist on the H2O cloud itself.  \n",
    "\n",
    "In this case, the H2O cluster is running on our laptops. Data files are imported by their relative locations to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parse Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "covtype_df = h2o.import_file(\"http://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/covtype/covtype.full.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the full covertype dataset (581k rows, 13 columns, 10 numerical, 3 categorical) and then split the data 3 ways:  \n",
    "  \n",
    "60% for training  \n",
    "20% for validation (hyper parameter tuning)  \n",
    "20% for final testing  \n",
    "\n",
    " We will train a data set on one set and use the others to test the validity of the model by ensuring that it can predict accurately on data the model has not been shown.  \n",
    " \n",
    " The second set will be used for validation most of the time.  \n",
    " \n",
    " The third set will be withheld until the end, to ensure that our validation accuracy is consistent with data we have never seen during the iterative process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split the data as described above\n",
    "train, valid, test = covtype_df.split_frame([0.7, 0.15], seed=1234)\n",
    "\n",
    "#Prepare predictors and response columns\n",
    "covtype_X = covtype_df.col_names[:-1]     #last column is Cover_Type, our desired response variable \n",
    "covtype_y = covtype_df.col_names[-1]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###The First Multinomial Model\n",
    "\n",
    "Our goal is to perform classification on cartographical data into tree cover categories.\n",
    "\n",
    "This is a multinomial problem, so let's begin by building a multinomial GLM model with default parameters!\n",
    "\n",
    "We will use the Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) algorithm to ensure that this demo can be run in almost all environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glm_multi_v1 = H2OGeneralizedLinearEstimator(\n",
    "                    model_id='glm_v1',            #allows us to easily locate this model in Flow\n",
    "                    family='multinomial',\n",
    "                    solver='L_BFGS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Model Construction\n",
    "H2O in Python is designed to be very similar in look and feel to to scikit-learn. Models are initialized individually with desired or default parameters and then trained on data.  \n",
    "\n",
    "**Note that the below example uses model.train() as opposed the traditional model.fit()**  \n",
    "This is because h2o-py takes column indices for the feature and response columns AND the whole data frame, while scikit-learn takes in a feature frame and a response frame.\n",
    "\n",
    "H2O supports model.fit() so that it can be incorporated into a scikit-learn pipeline, but we advise using train() in all other cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "glm Model Build Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "glm_multi_v1.train(covtype_X, covtype_y, training_frame=train, validation_frame=valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view information about the model in [Flow](http://localhost:54321/) or within Python. To find more information in Flow, enter `getModel \"rf_covType_v1\"` into a cell and run in place pressing Ctrl-Enter. Alternatively, you can click on the Models tab, select List All Models, and click on the model named \"rf_covType_v1\" as specified in our model construction above.\n",
    "\n",
    "In Python, we can use call the model itself to get an overview of its stats,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OGeneralizedLinearEstimator :  Generalized Linear Model\n",
      "Model Key:  glm_v1\n",
      "\n",
      "GLM Model: summary\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>family</b></td>\n",
       "<td><b>link</b></td>\n",
       "<td><b>regularization</b></td>\n",
       "<td><b>number_of_predictors_total</b></td>\n",
       "<td><b>number_of_active_predictors</b></td>\n",
       "<td><b>number_of_iterations</b></td>\n",
       "<td><b>training_frame</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>multinomial</td>\n",
       "<td>multinomial</td>\n",
       "<td>Ridge ( lambda = 0.2206 )</td>\n",
       "<td>385</td>\n",
       "<td>384</td>\n",
       "<td>2</td>\n",
       "<td>py_2</td></tr></table></div>"
      ],
      "text/plain": [
       "    family       link         regularization             number_of_predictors_total    number_of_active_predictors    number_of_iterations    training_frame\n",
       "--  -----------  -----------  -------------------------  ----------------------------  -----------------------------  ----------------------  ----------------\n",
       "    multinomial  multinomial  Ridge ( lambda = 0.2206 )  385                           384                            2                       py_2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsMultinomialGLM: glm\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.410897794278\n",
      "\n",
      "ModelMetricsMultinomialGLM: glm\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.410773969715\n",
      "\n",
      "Scoring History:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>iteration</b></td>\n",
       "<td><b>log_likelihood</b></td>\n",
       "<td><b>objective</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2015-11-07 06:57:44</td>\n",
       "<td> 0.000 sec</td>\n",
       "<td>0</td>\n",
       "<td>489957.4</td>\n",
       "<td>1.2</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2015-11-07 06:57:45</td>\n",
       "<td> 1.512 sec</td>\n",
       "<td>1</td>\n",
       "<td>489957.4</td>\n",
       "<td>1.2</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2015-11-07 06:57:45</td>\n",
       "<td> 1.595 sec</td>\n",
       "<td>2</td>\n",
       "<td>489957.4</td>\n",
       "<td>1.2</td></tr></table></div>"
      ],
      "text/plain": [
       "    timestamp            duration    iteration    log_likelihood    objective\n",
       "--  -------------------  ----------  -----------  ----------------  -----------\n",
       "    2015-11-07 06:57:44  0.000 sec   0            489957            1.20537\n",
       "    2015-11-07 06:57:45  1.512 sec   1            489957            1.20537\n",
       "    2015-11-07 06:57:45  1.595 sec   2            489957            1.20537"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm_multi_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out a little more about its performance, we can look at its hit ratio table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-7 Hit Ratios:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>k</b></td>\n",
       "<td><b>hit_ratio</b></td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>0.5</td></tr>\n",
       "<tr><td>2</td>\n",
       "<td>0.8520448</td></tr>\n",
       "<tr><td>3</td>\n",
       "<td>0.9129493</td></tr>\n",
       "<tr><td>4</td>\n",
       "<td>0.9</td></tr>\n",
       "<tr><td>5</td>\n",
       "<td>0.9793167</td></tr>\n",
       "<tr><td>6</td>\n",
       "<td>0.995167</td></tr>\n",
       "<tr><td>7</td>\n",
       "<td>1.0</td></tr></table></div>"
      ],
      "text/plain": [
       "k    hit_ratio\n",
       "---  -----------\n",
       "1    0.489286\n",
       "2    0.852045\n",
       "3    0.912949\n",
       "4    0.949219\n",
       "5    0.979317\n",
       "6    0.995167\n",
       "7    1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm_multi_v1.hit_ratio_table(valid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Updating our GLM Estimator\n",
    "As we can see, the k=1 hit ratio indicates that we're very far off of a good estimator. Judging by our training and validation scores, we don't seem to be overfitting. Perhaps we're over-regularizing?\n",
    "\n",
    "Let's try again with a lower lambda value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "glm Model Build Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "glm_multi_v2 = H2OGeneralizedLinearEstimator(\n",
    "                    model_id='glm_v2',           \n",
    "                    family='multinomial',\n",
    "                    solver='L_BFGS',\n",
    "                    Lambda=0.0001                 #default value 0.001\n",
    ")\n",
    "glm_multi_v2.train(covtype_X, covtype_y, training_frame=train, validation_frame=valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OGeneralizedLinearEstimator :  Generalized Linear Model\n",
      "Model Key:  glm_v2\n",
      "\n",
      "GLM Model: summary\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>family</b></td>\n",
       "<td><b>link</b></td>\n",
       "<td><b>regularization</b></td>\n",
       "<td><b>number_of_predictors_total</b></td>\n",
       "<td><b>number_of_active_predictors</b></td>\n",
       "<td><b>number_of_iterations</b></td>\n",
       "<td><b>training_frame</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>multinomial</td>\n",
       "<td>multinomial</td>\n",
       "<td>Ridge ( lambda = 1.0E-4 )</td>\n",
       "<td>385</td>\n",
       "<td>384</td>\n",
       "<td>73</td>\n",
       "<td>py_2</td></tr></table></div>"
      ],
      "text/plain": [
       "    family       link         regularization             number_of_predictors_total    number_of_active_predictors    number_of_iterations    training_frame\n",
       "--  -----------  -----------  -------------------------  ----------------------------  -----------------------------  ----------------------  ----------------\n",
       "    multinomial  multinomial  Ridge ( lambda = 1.0E-4 )  385                           384                            73                      py_2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsMultinomialGLM: glm\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.207377440287\n",
      "\n",
      "ModelMetricsMultinomialGLM: glm\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.20654270428\n",
      "\n",
      "Scoring History:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>iteration</b></td>\n",
       "<td><b>log_likelihood</b></td>\n",
       "<td><b>objective</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2015-11-07 06:58:01</td>\n",
       "<td> 0.000 sec</td>\n",
       "<td>0</td>\n",
       "<td>489957.4</td>\n",
       "<td>1.2</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2015-11-07 06:58:01</td>\n",
       "<td> 0.148 sec</td>\n",
       "<td>1</td>\n",
       "<td>389920.1</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2015-11-07 06:58:01</td>\n",
       "<td> 0.303 sec</td>\n",
       "<td>2</td>\n",
       "<td>364729.6</td>\n",
       "<td>0.9</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2015-11-07 06:58:01</td>\n",
       "<td> 0.457 sec</td>\n",
       "<td>3</td>\n",
       "<td>340192.4</td>\n",
       "<td>0.8</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2015-11-07 06:58:01</td>\n",
       "<td> 0.534 sec</td>\n",
       "<td>4</td>\n",
       "<td>324275.6</td>\n",
       "<td>0.8</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2015-11-07 06:58:01</td>\n",
       "<td> 0.688 sec</td>\n",
       "<td>5</td>\n",
       "<td>310063.9</td>\n",
       "<td>0.8</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2015-11-07 06:58:01</td>\n",
       "<td> 0.844 sec</td>\n",
       "<td>6</td>\n",
       "<td>297327.2</td>\n",
       "<td>0.7</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2015-11-07 06:58:02</td>\n",
       "<td> 0.998 sec</td>\n",
       "<td>7</td>\n",
       "<td>290358.7</td>\n",
       "<td>0.7</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2015-11-07 06:58:02</td>\n",
       "<td> 1.149 sec</td>\n",
       "<td>8</td>\n",
       "<td>284386.5</td>\n",
       "<td>0.7</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2015-11-07 06:58:03</td>\n",
       "<td> 2.208 sec</td>\n",
       "<td>16</td>\n",
       "<td>265005.1</td>\n",
       "<td>0.7</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2015-11-07 06:58:04</td>\n",
       "<td> 3.350 sec</td>\n",
       "<td>24</td>\n",
       "<td>260400.7</td>\n",
       "<td>0.6</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2015-11-07 06:58:05</td>\n",
       "<td> 4.584 sec</td>\n",
       "<td>32</td>\n",
       "<td>259036.0</td>\n",
       "<td>0.6</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2015-11-07 06:58:06</td>\n",
       "<td> 5.830 sec</td>\n",
       "<td>40</td>\n",
       "<td>258304.9</td>\n",
       "<td>0.6</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2015-11-07 06:58:07</td>\n",
       "<td> 6.816 sec</td>\n",
       "<td>48</td>\n",
       "<td>257953.9</td>\n",
       "<td>0.6</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2015-11-07 06:58:09</td>\n",
       "<td> 8.033 sec</td>\n",
       "<td>56</td>\n",
       "<td>257701.4</td>\n",
       "<td>0.6</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2015-11-07 06:58:10</td>\n",
       "<td> 9.176 sec</td>\n",
       "<td>64</td>\n",
       "<td>257629.6</td>\n",
       "<td>0.6</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2015-11-07 06:58:11</td>\n",
       "<td>10.321 sec</td>\n",
       "<td>72</td>\n",
       "<td>257506.3</td>\n",
       "<td>0.6</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2015-11-07 06:58:11</td>\n",
       "<td>10.402 sec</td>\n",
       "<td>73</td>\n",
       "<td>257506.3</td>\n",
       "<td>0.6</td></tr></table></div>"
      ],
      "text/plain": [
       "    timestamp            duration    iteration    log_likelihood    objective\n",
       "--  -------------------  ----------  -----------  ----------------  -----------\n",
       "    2015-11-07 06:58:01  0.000 sec   0            489957            1.20537\n",
       "    2015-11-07 06:58:01  0.148 sec   1            389920            0.959361\n",
       "    2015-11-07 06:58:01  0.303 sec   2            364730            0.897496\n",
       "    2015-11-07 06:58:01  0.457 sec   3            340192            0.837365\n",
       "    2015-11-07 06:58:01  0.534 sec   4            324276            0.798494\n",
       "    2015-11-07 06:58:01  0.688 sec   5            310064            0.76399\n",
       "    2015-11-07 06:58:01  0.844 sec   6            297327            0.733131\n",
       "    2015-11-07 06:58:02  0.998 sec   7            290359            0.716153\n",
       "    2015-11-07 06:58:02  1.149 sec   8            284386            0.701728\n",
       "    2015-11-07 06:58:03  2.208 sec   16           265005            0.657024\n",
       "    2015-11-07 06:58:04  3.350 sec   24           260401            0.647857\n",
       "    2015-11-07 06:58:05  4.584 sec   32           259036            0.645447\n",
       "    2015-11-07 06:58:06  5.830 sec   40           258305            0.64409\n",
       "    2015-11-07 06:58:07  6.816 sec   48           257954            0.64364\n",
       "    2015-11-07 06:58:09  8.033 sec   56           257701            0.643252\n",
       "    2015-11-07 06:58:10  9.176 sec   64           257630            0.643105\n",
       "    2015-11-07 06:58:11  10.321 sec  72           257506            0.642981\n",
       "    2015-11-07 06:58:11  10.402 sec  73           257506            0.642981"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm_multi_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-7 Hit Ratios:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>k</b></td>\n",
       "<td><b>hit_ratio</b></td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>0.724647</td></tr>\n",
       "<tr><td>2</td>\n",
       "<td>0.968643</td></tr>\n",
       "<tr><td>3</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>4</td>\n",
       "<td>0.99795</td></tr>\n",
       "<tr><td>5</td>\n",
       "<td>0.9997366</td></tr>\n",
       "<tr><td>6</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>7</td>\n",
       "<td>1.0</td></tr></table></div>"
      ],
      "text/plain": [
       "k    hit_ratio\n",
       "---  -----------\n",
       "1    0.724647\n",
       "2    0.968643\n",
       "3    0.994113\n",
       "4    0.99795\n",
       "5    0.999737\n",
       "6    0.999989\n",
       "7    1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm_multi_v2.hit_ratio_table(valid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a noticeable improvement in the MSE, and our hit ratio has improved from coin-flip to 72%. \n",
    "\n",
    "Let's look at the confusion matrix to see if we can gather any more insight on the errors in our multinomial classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix: vertical: actual; across: predicted\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>class_1</b></td>\n",
       "<td><b>class_2</b></td>\n",
       "<td><b>class_3</b></td>\n",
       "<td><b>class_4</b></td>\n",
       "<td><b>class_5</b></td>\n",
       "<td><b>class_6</b></td>\n",
       "<td><b>class_7</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>22183.0</td>\n",
       "<td>8871.0</td>\n",
       "<td>5.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>11.0</td>\n",
       "<td>605.0</td>\n",
       "<td>0.3</td>\n",
       "<td>9,492 / 31,675</td></tr>\n",
       "<tr><td>7736.0</td>\n",
       "<td>34156.0</td>\n",
       "<td>546.0</td>\n",
       "<td>1.0</td>\n",
       "<td>27.0</td>\n",
       "<td>238.0</td>\n",
       "<td>19.0</td>\n",
       "<td>0.2</td>\n",
       "<td>8,567 / 42,723</td></tr>\n",
       "<tr><td>0.0</td>\n",
       "<td>532.0</td>\n",
       "<td>4368.0</td>\n",
       "<td>69.0</td>\n",
       "<td>1.0</td>\n",
       "<td>348.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.2</td>\n",
       "<td>950 / 5,318</td></tr>\n",
       "<tr><td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>222.0</td>\n",
       "<td>133.0</td>\n",
       "<td>0.0</td>\n",
       "<td>67.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.7</td>\n",
       "<td>289 / 422</td></tr>\n",
       "<tr><td>9.0</td>\n",
       "<td>1324.0</td>\n",
       "<td>39.0</td>\n",
       "<td>0.0</td>\n",
       "<td>5.0</td>\n",
       "<td>7.0</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1,379 / 1,384</td></tr>\n",
       "<tr><td>0.0</td>\n",
       "<td>613.0</td>\n",
       "<td>1365.0</td>\n",
       "<td>4.0</td>\n",
       "<td>0.0</td>\n",
       "<td>646.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.8</td>\n",
       "<td>1,982 / 2,628</td></tr>\n",
       "<tr><td>1350.0</td>\n",
       "<td>34.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>1783.0</td>\n",
       "<td>0.4</td>\n",
       "<td>1,384 / 3,167</td></tr>\n",
       "<tr><td>31278.0</td>\n",
       "<td>45530.0</td>\n",
       "<td>6545.0</td>\n",
       "<td>207.0</td>\n",
       "<td>33.0</td>\n",
       "<td>1317.0</td>\n",
       "<td>2407.0</td>\n",
       "<td>0.3</td>\n",
       "<td>24,043 / 87,317</td></tr></table></div>"
      ],
      "text/plain": [
       "class_1    class_2    class_3    class_4    class_5    class_6    class_7    Error     Rate\n",
       "---------  ---------  ---------  ---------  ---------  ---------  ---------  --------  ---------------\n",
       "22183      8871       5          0          0          11         605        0.299669  9,492 / 31,675\n",
       "7736       34156      546        1          27         238        19         0.200524  8,567 / 42,723\n",
       "0          532        4368       69         1          348        0          0.178639  950 / 5,318\n",
       "0          0          222        133        0          67         0          0.684834  289 / 422\n",
       "9          1324       39         0          5          7          0          0.996387  1,379 / 1,384\n",
       "0          613        1365       4          0          646        0          0.754186  1,982 / 2,628\n",
       "1350       34         0          0          0          0          1783       0.437007  1,384 / 3,167\n",
       "31278      45530      6545       207        33         1317       2407       0.275353  24,043 / 87,317"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm_multi_v2.confusion_matrix(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class 1 & 2 Struggles\n",
    "\n",
    "As we can see in the above confusion matrix, our model is struggling to correctly distinguish between covertype classes 1 and 2. To learn more about this, let's shrink the scope of our problem to a binomial classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Binomial Classification\n",
    "\n",
    "Let's only look at the rows where coverage is class_1 or class_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c1 = covtype_df[covtype_df['Cover_Type'] == 'class_1']\n",
    "c2 = covtype_df[covtype_df['Cover_Type'] == 'class_2']\n",
    "df_b = c1.rbind(c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, let's split this into train, valid and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split the data as described above\n",
    "train_b, valid_b, test_b = df_b.split_frame([0.7, 0.15], seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a binomial classifier with the default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "glm Model Build Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "glm_binom_v1 = H2OGeneralizedLinearEstimator(\n",
    "                    model_id=\"glm_v3\",\n",
    "                    solver=\"L_BFGS\",\n",
    "                    family=\"binomial\")\n",
    "glm_binom_v1.train(covtype_X, covtype_y, training_frame=train_b, validation_frame=valid_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5139641167216168, 0.7764640727803466]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm_binom_v1.accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the data in its natural state does not classify particularly cleanly into class_1 or class_2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Featurization\n",
    "\n",
    "Let's add some features to this binomial model to see if we can improve its predictive capacity. We'll do a combination of binning (by converting several numeric fields to categorical) and interaction variables. To do this cleanly, we use the two helper functions defined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cut_column(train_df, train, valid, test, col):\n",
    "    '''\n",
    "    Convenience function to change a column from numerical to categorical\n",
    "    We use train_df only for bucketing with histograms.\n",
    "    Uses np.histogram to generate a histogram, with the buckets forming the categories of our new categorical.\n",
    "    Picks buckets based on training data, then applies the same classification to the test and validation sets\n",
    "    \n",
    "    Assumes that train, valid, test will have the same histogram behavior.\n",
    "    '''\n",
    "    only_col= train_df[col]                            #Isolate the column in question from the training frame\n",
    "    counts, breaks = np.histogram(only_col, bins=20)   #Generate counts and breaks for our histogram\n",
    "    min_val = min(only_col)-1                          #Establish min and max values\n",
    "    max_val = max(only_col)+1\n",
    "    \n",
    "    new_b = [min_val]                                  #Redefine breaks such that each bucket has enough support\n",
    "    for i in xrange(19):\n",
    "        if counts[i] > 1000 and counts[i+1] > 1000:\n",
    "            new_b.append(breaks[i+1])\n",
    "    new_b.append(max_val)\n",
    "    \n",
    "    names = [col + '_' + str(x) for x in xrange(len(new_b)-1)]  #Generate names for buckets, these will be categorical names\n",
    "\n",
    "    train[col+\"_cut\"] = train[col].cut(breaks=new_b, labels=names)\n",
    "    valid[col+\"_cut\"] = valid[col].cut(breaks=new_b, labels=names)\n",
    "    test[col+\"_cut\"] = test[col].cut(breaks=new_b, labels=names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_features(train, valid, test):\n",
    "    '''\n",
    "    Helper function to add a specific set of features to our covertype dataset\n",
    "    '''\n",
    "    #pull train dataset into Python\n",
    "    train_df = train.as_data_frame(True)\n",
    "    \n",
    "    #Make categoricals for several columns\n",
    "    cut_column(train_df, train, valid, test, \"Elevation\")\n",
    "    cut_column(train_df, train, valid, test, \"Hillshade_Noon\")\n",
    "    cut_column(train_df, train, valid, test, \"Hillshade_9am\")\n",
    "    cut_column(train_df, train, valid, test, \"Hillshade_3pm\")\n",
    "    cut_column(train_df, train, valid, test, \"Horizontal_Distance_To_Hydrology\")\n",
    "    cut_column(train_df, train, valid, test, \"Slope\")\n",
    "    cut_column(train_df, train, valid, test, \"Horizontal_Distance_To_Roadways\")\n",
    "    cut_column(train_df, train, valid, test, \"Aspect\")\n",
    "    \n",
    "    \n",
    "    #Add interaction columns for a subset of columns\n",
    "    interaction_cols1 = [\"Elevation_cut\",\n",
    "                         \"Wilderness_Area\",\n",
    "                         \"Soil_Type\",\n",
    "                         \"Hillshade_Noon_cut\",\n",
    "                         \"Hillshade_9am_cut\",\n",
    "                         \"Hillshade_3pm_cut\",\n",
    "                         \"Horizontal_Distance_To_Hydrology_cut\",\n",
    "                         \"Slope_cut\",\n",
    "                         \"Horizontal_Distance_To_Roadways_cut\",\n",
    "                         \"Aspect_cut\"]\n",
    "\n",
    "    train_cols = train.interaction(factors=interaction_cols1,    #Generate pairwise columns\n",
    "                                   pairwise=True,\n",
    "                                   max_factors=1000,\n",
    "                                   min_occurrence=100,\n",
    "                                   destination_frame=\"itrain\")\n",
    "    valid_cols = valid.interaction(factors=interaction_cols1,\n",
    "                                   pairwise=True,\n",
    "                                   max_factors=1000,\n",
    "                                   min_occurrence=100,\n",
    "                                   destination_frame=\"ivalid\")\n",
    "    test_cols = test.interaction(factors=interaction_cols1,\n",
    "                                   pairwise=True,\n",
    "                                   max_factors=1000,\n",
    "                                   min_occurrence=100,\n",
    "                                   destination_frame=\"itest\")\n",
    "    \n",
    "    train = train.cbind(train_cols)                              #Append pairwise columns to H2OFrames\n",
    "    valid = valid.cbind(valid_cols)\n",
    "    test = test.cbind(test_cols)\n",
    "    \n",
    "    \n",
    "    #Add a three-way interaction for Hillshade\n",
    "    interaction_cols2 = [\"Hillshade_Noon_cut\",\"Hillshade_9am_cut\",\"Hillshade_3pm_cut\"]\n",
    "    \n",
    "    train_cols = train.interaction(factors=interaction_cols2,    #Generate pairwise columns\n",
    "                                   pairwise=False,\n",
    "                                   max_factors=1000,\n",
    "                                   min_occurrence=100,\n",
    "                                   destination_frame=\"itrain\")\n",
    "    valid_cols = valid.interaction(factors=interaction_cols2,\n",
    "                                   pairwise=False,\n",
    "                                   max_factors=1000,\n",
    "                                   min_occurrence=100,\n",
    "                                   destination_frame=\"ivalid\")\n",
    "    test_cols = test.interaction(factors=interaction_cols2,\n",
    "                                   pairwise=False,\n",
    "                                   max_factors=1000,\n",
    "                                   min_occurrence=100,\n",
    "                                   destination_frame=\"itest\")\n",
    "    \n",
    "    train = train.cbind(train_cols)                              #Append pairwise columns to H2OFrames\n",
    "    valid = valid.cbind(valid_cols)\n",
    "    test = test.cbind(test_cols)\n",
    "    \n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Add features to our binomial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Interactions Progress: [##################################################] 100%\n",
      "\n",
      "Interactions Progress: [##################################################] 100%\n",
      "\n",
      "Interactions Progress: [##################################################] 100%\n",
      "\n",
      "Interactions Progress: [##################################################] 100%\n",
      "\n",
      "Interactions Progress: [##################################################] 100%\n",
      "\n",
      "Interactions Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "train_bf, valid_bf, test_bf = add_features(train_b, valid_b, test_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "glm Model Build Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "glm_binom_feat_1 = H2OGeneralizedLinearEstimator(family='binomial', solver='L_BFGS', model_id='glm_v4')\n",
    "glm_binom_feat_1.train(covtype_X, covtype_y, training_frame=train_bf, validation_frame=valid_bf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5364701476265027, 0.7635187824356665]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm_binom_feat_1.accuracy(valid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We appear to have marginal improvement in accuracy! Inspecting in flow, we see that we may be over-regularizing like in our very first model, so we once again decrement lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "glm Model Build Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "glm_binom_feat_2 = H2OGeneralizedLinearEstimator(family='binomial', solver='L_BFGS', model_id='glm_v5', Lambda=0.001)\n",
    "glm_binom_feat_2.train(covtype_X, covtype_y, training_frame=train_bf, validation_frame=valid_bf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5119004481036479, 0.7763048213181317]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm_binom_feat_2.accuracy(valid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Validation accuracy is increasing! Let's try adding in lambda search to see if we can possibly improve any further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "glm Model Build Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "glm_binom_feat_3 = H2OGeneralizedLinearEstimator(family='binomial', model_id='glm_v6', lambda_search=True)\n",
    "glm_binom_feat_3.train(covtype_X, covtype_y, training_frame=train_bf, validation_frame=valid_bf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5349735444445648, 0.7765737180349028]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm_binom_feat_3.accuracy(valid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This yields minimal improvements over lambda=0.001. Thus, we can conclude that the optimal lambda value is quite close to 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Revisiting the Multinomial\n",
    "\n",
    "We've managed to reduce the error in classification between class_1 and class_2 by adding some features and categorizing others. Let's apply these changes to our original multinomial model to see what sorts of gains we can achieve. First let's featurize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Interactions Progress: [##################################################] 100%\n",
      "\n",
      "Interactions Progress: [##################################################] 100%\n",
      "\n",
      "Interactions Progress: [##################################################] 100%\n",
      "\n",
      "Interactions Progress: [##################################################] 100%\n",
      "\n",
      "Interactions Progress: [##################################################] 100%\n",
      "\n",
      "Interactions Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "train_f, valid_f, test_f = add_features(train, valid, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a final multinomial classifier with our featurized data and a near-optimal lambda of 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "glm Model Build Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "glm_multi_v3 = H2OGeneralizedLinearEstimator(\n",
    "                    model_id='glm_v7',           \n",
    "                    family='multinomial',\n",
    "                    solver='L_BFGS',\n",
    "                    Lambda=0.0001)\n",
    "glm_multi_v3.train(covtype_X, covtype_y, training_frame=train_f, validation_frame=valid_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-7 Hit Ratios:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>k</b></td>\n",
       "<td><b>hit_ratio</b></td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>0.724647</td></tr>\n",
       "<tr><td>2</td>\n",
       "<td>0.968643</td></tr>\n",
       "<tr><td>3</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>4</td>\n",
       "<td>0.99795</td></tr>\n",
       "<tr><td>5</td>\n",
       "<td>0.9997366</td></tr>\n",
       "<tr><td>6</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>7</td>\n",
       "<td>1.0</td></tr></table></div>"
      ],
      "text/plain": [
       "k    hit_ratio\n",
       "---  -----------\n",
       "1    0.724647\n",
       "2    0.968643\n",
       "3    0.994113\n",
       "4    0.99795\n",
       "5    0.999737\n",
       "6    0.999989\n",
       "7    1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm_multi_v3.hit_ratio_table(valid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our hit ratio has improved dramatically since our first multinomial!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More information can be found in the [H2O Generalized Linear Modeling Booklet](http://h2o.ai/resources/), in our [H2O SlideShare Presentations](http://www.slideshare.net/0xdata/presentations), our [H2O YouTube channel](https://www.youtube.com/user/0xdata/), as well as on our [H2O Github Repository](https://github.com/h2oai/h2o-3/), especially in our [H2O GLM R tests](https://github.com/h2oai/h2o-3/tree/master/h2o-r/tests/testdir_algos), and [H2O GLM tests](https://github.com/h2oai/h2o-3/tree/master/h2o-py/tests/testdir_algos/glm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
